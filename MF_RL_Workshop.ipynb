{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MF_RL_Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhejna/rlworkshop/blob/main/MF_RL_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TofUv8vEed"
      },
      "source": [
        "# 0. Setup\n",
        "Run the following cells to test the installation and make sure everything is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg7qA4TwvAUt",
        "outputId": "c7e7b106-3fc0-46e3-fef4-38e2ca9c9d63"
      },
      "source": [
        "# Check if we have GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU working!\")\n",
        "else:\n",
        "    print(\"WARNING: The Colab runtime does not have GPU enabled.\")\n",
        "    print(\"         You should be able to train the models on CPU, but if you\")\n",
        "    print(\"         want GPU, enable it in Runtime > Change Runtime Type\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: The Colab runtime does not have GPU enabled.\n",
            "         You should be able to train the models on CPU, but if you\n",
            "         want GPU, enable it in Runtime > Change Runtime Type\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKmwVkibl2ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2298ecde-8fd9-4083-867a-5cec8fc16dd3"
      },
      "source": [
        "%%bash\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,977 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n",
            "Fetched 993 kB in 2s (506 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144793 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\r\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Selecting previously unselected package x11-utils.\r\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\r\n",
            "Unpacking x11-utils (7.7+3build1) ...\r\n",
            "Selecting previously unselected package xvfb.\r\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\r\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\r\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\r\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Setting up x11-utils (7.7+3build1) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cttFN8FfrpV0"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(640, 480))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6QcBFWArfxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4362af-0701-4766-d460-2d3b37ef51ba"
      },
      "source": [
        "# This should produce a non-blank output.\n",
        "!echo $DISPLAY"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ":0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5tV_P3Pl9eh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "ec4b5d4a-73ec-484e-ceb8-1bc5a6a15df0"
      },
      "source": [
        "# Test rendering a gym environment\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "num_ep = 0\n",
        "obs = env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "for _ in range(100):\n",
        "    ac = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(ac)\n",
        "    img.set_data(env.render(mode='rgb_array')) \n",
        "    plt.axis('off')\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        num_ep += 1\n",
        "env.close()\n",
        "del env\n",
        "del img\n",
        "print(\"Episodes:\", num_ep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUmNsgmMvtla"
      },
      "source": [
        "# 1. REINFORCE\n",
        "Here we'll implement the basic policy gradient algorithm REINFORCE in pytorch for the above cart-pole environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V80xIEMvuxz"
      },
      "source": [
        "# Import the required modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYtOXM5in-cr"
      },
      "source": [
        "## a) Policy Network\n",
        "First we need to implement the policy network. The policy network takes in observations and outputs actions that should be taken. Our policy will be implemented in the form of an MLP.\n",
        "\n",
        "We're deliberately providing less skeleton code here, since this is one of the later workshops. It's a great opportunity to practice designing parts of your network and Googling the appropriate Pytorch (or Numpy, if needed) methods/classes that you'll need to implement it. That said, if you get stuck, definitely ask other workshop attendees or presenters for help!\n",
        "\n",
        "Feel free to structure your network however you want, but a simple network (the solution just uses linear layers with ReLU activations) will do fine as CartPole is not too difficult an environment for a network to learn.\n",
        "\n",
        "Our policy class must do the following:\n",
        "1. Take in an observation from the environment, and return the action to take.\n",
        "2. Given an observation and action, compute the log probability of taking that action under the current policy. This is used for computing the REINFORCE loss.\n",
        "\n",
        "There are a few considerations to keep in mind:\n",
        "\n",
        "**Discrete Actions**: The cartpole environment has a discrete action space: the cart can decide to move left or right, but can't choose how fast to move. This means we will need to output a categorical distribution. Think about the best way to implement this. (Hint: treat it like a classification problem). Your solution should ideally work for a discrete action space of any size, even though CartPole has only two actions.\n",
        "\n",
        "**Exploration**: Remember that for RL to work, we need to explore. Make sure you have some randomness in your action selection, though it might be worth having a deterministic option that chooses the best known policy that the model can use at test time.\n",
        "\n",
        "Define your class below.\n",
        "\n",
        "Hints:\n",
        "- Pytorch `Sequential` can be used to create networks where the output of each layer is fed into the next layer\n",
        "- Remember, networks should inherit from `nn.Module` and implement crucial functions such as `__init__` and `forward`. Any such network can be used as the building block of another network -- feel free to create \"helper\" networks if you think they would be useful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7e7SzZwExu"
      },
      "source": [
        "# Define your network here!\n",
        "# Feel free to do whatever you want, but we have provided a suggested interface\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    pass\n",
        "\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    # TODO: Fill in __init__\n",
        "\n",
        "    # TODO: perhaps fill in forward\n",
        "\n",
        "    def get_action(self, obs, deterministic=False):\n",
        "        # Remember, we need to be able to explore!\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def log_prob(self, action, obs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# NOTE: Remember that one thing you will need to compute when coding policy gradients is the\n",
        "#       log probability of taking a given action from a state. Make sure your code can compute\n",
        "#       compute this!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj7X8HkrxyOU"
      },
      "source": [
        "## b) Training Loop\n",
        "Next, we need to actually train the policy network. This is done in two phases:\n",
        "1. Data Collection: execute the policy in the enviornment and explore, collecting experience. For this you may need to refer to the gym environment interface. See how we did it in the setup or check out the documentation here: https://gym.openai.com/docs/\n",
        "2. Update the policy: use the collected experience to update the policy using the computed policy gradient.\n",
        "\n",
        "We have provided some skeleton code for the training loop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvV0Zj0NxzZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a19cf71d-dc2a-460d-8eaa-28f8758ceb39"
      },
      "source": [
        "# Hyper-parameters\n",
        "iterations = 64\n",
        "epochs_per_iter = 4\n",
        "timesteps_per_iter = 2048\n",
        "batch_size = 512\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99 # This is the discount factor\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "obs = env.reset()\n",
        "\n",
        "# Initialize the policy network\n",
        "policy = ... # YOUR CODE HERE\n",
        "\n",
        "# Initialize the optimizer\n",
        "optim = ... # YOUR CODE HERE\n",
        "\n",
        "for iter in range(iterations):\n",
        "    # First, collect data\n",
        "    # Setup the data collection buffers\n",
        "    observations = np.zeros((timesteps_per_iter, *env.observation_space.shape), dtype=np.float)\n",
        "    actions = np.zeros((timesteps_per_iter, 1), np.float)\n",
        "    rewards = np.zeros(timesteps_per_iter, np.float)\n",
        "    dones = np.zeros(timesteps_per_iter, dtype=np.bool)\n",
        "    \n",
        "    # Collect the data\n",
        "    for i in range(timesteps_per_iter):\n",
        "        action = ... # YOUR CODE HERE\n",
        "        # Record the current observation and action\n",
        "        # YOUR CODE HERE: remember the buffers we made earlier.\n",
        "\n",
        "        # Step the environment to observe the consequences of your actions\n",
        "        # YOUR CODE HERE: hint - how do we update the environment with an action\n",
        "\n",
        "        # Record the consequences of your actions\n",
        "        # YOUR CODE HERE: hint - what are the other two buffers?\n",
        "\n",
        "        # Remember to check for resets!\n",
        "        if False: # REPLACE ME\n",
        "            # YOUR CODE HERE: Remember to check for resets!\n",
        "            pass # Replace me!\n",
        "\n",
        "    print(\"Average Reward:\", np.sum(rewards) / np.sum(dones))\n",
        "\n",
        "    # Next, we need to preprocess our rewards to convert them to advantages.\n",
        "    # Compute the reward to go\n",
        "    reward_to_go = np.zeros(timesteps_per_iter)\n",
        "\n",
        "    # Now, you must compute the reward to go. This is just the sum of _discounted_\n",
        "    # rewards from the current time step i to _the end of the episode_!\n",
        "    # You will need to account for the fact that there are multiple episodes in\n",
        "    # the buffers we have created. Use the done variable to determine when an episode is over.\n",
        "    # This is a very important part of the code -- if this is slightly off it won't work at all.\n",
        "    # YOUR CODE HERE:\n",
        "\n",
        "    # Compute the advantage\n",
        "    # YOUR CODE HERE: For this basic approach, consider the simple mean method.\n",
        "    advantages = ...\n",
        "\n",
        "\n",
        "    # Convert everything to pytorch objects now for efficiency\n",
        "    observations = torch.from_numpy(observations).float()\n",
        "    actions = torch.from_numpy(actions).long()\n",
        "    advantages = torch.from_numpy(advantages).float()\n",
        "    \n",
        "    # Update the policy\n",
        "    for epoch in range(epochs_per_iter):\n",
        "        indices = np.random.permutation(timesteps_per_iter)\n",
        "        # Note this is a bit sloppy as we don't necesarily hit all data points if batch does not evenly divide timesteps \n",
        "        for i in range(timesteps_per_iter // batch_size): \n",
        "            batch_indices = indices[i*batch_size:(i+1)*batch_size]\n",
        "            batch_obs, batch_actions, batch_advantages = observations[batch_indices], actions[batch_indices], advantages[batch_indices]\n",
        "            # Now update the policy\n",
        "            optim.zero_grad()            \n",
        "            log_probs = ... # YOUR CODE HERE. Get log probability from the network\n",
        "            assert len(log_probs.shape) == 1, \"Each timestep has one prediction!\"\n",
        "            loss = ... # YOUR CODE HERE. Compute the policy loss. Hint - think about what this quantity should be so its gradient is the policy gradient.\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "    print(\"Policy Loss:\", loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Reward: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c56a06524b78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Update the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got ellipsis)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNd7kXi4kJon"
      },
      "source": [
        "## c) Testing!\n",
        "RL, especially regular REINFORCE, is very unstable and sensitive to hyper-parameters. Try tuning the above algorithm  by changing hyper-parameters until you can get an average final reward of > 500 (This is possible!).\n",
        "\n",
        "Once you have a functioning policy, execute it in the environment and watch the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRIWdI2ecq57"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "eval_eps = 5\n",
        "ep_rewards = []\n",
        "obs = env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "while eval_eps > 0:\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    ep_reward = 0\n",
        "    while not done:\n",
        "        aciton = ... # YOUR CODE HERE\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        # Render\n",
        "        img.set_data(env.render(mode='rgb_array')) \n",
        "        plt.axis('off')\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "    ep_rewards.append(ep_reward)\n",
        "    eval_eps -= 1\n",
        "\n",
        "print(\"Average Eval Reward\", sum(ep_rewards) / len(ep_rewards))\n",
        "env.close()\n",
        "del env\n",
        "del img\n",
        "print(\"Episodes:\", num_ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WTW9B0Ep30d"
      },
      "source": [
        "# 2. DDPG\n",
        "We previously implemented REINFORCE, the basic policy gradient method. Now we'll get fancier and implement DDPG.\n",
        "\n",
        "DDPG, or deep-deterministic policy gradients is an off policy Q-learning method for continuous action space environments. The paper can be found here: https://arxiv.org/abs/1509.02971. \n",
        "\n",
        "Here's a good summary of DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
        "\n",
        "\n",
        "DDPG has the following basic ingredients:\n",
        "1. A Q function that predicts the reward of different actions. Input = action + observation\n",
        "2. A target Q function that is the EMA of the regular Q function for regression targets\n",
        "3. A policy network that tries to learn to propose actions that maximize the Q function. As we are in continuous space, we can't simply take the argmax of the Q values.\n",
        "\n",
        "The code will be slightly re-factored in comparison to last time. We'll be using the pendulum environment because it is continuous instead of discrete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtGt3pY1BAp"
      },
      "source": [
        "# Import the required modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXemO56fzFMY"
      },
      "source": [
        "## a) Actor-Critic Networks\n",
        "The first step is to implement the actor and critic models. For convenience I have provided a `space_size` function that will tell you the size of a gym space.\n",
        "\n",
        "There's also "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHij7xvylEKl"
      },
      "source": [
        "def space_size(space):\n",
        "    if isinstance(space, gym.spaces.Box):\n",
        "        assert len(space.shape) == 1\n",
        "        return space.shape[0]\n",
        "    elif isinstance(space, gym.spaces.Dict):\n",
        "        total = 0\n",
        "        for k, v in space.spaces.items():\n",
        "            assert len(space.spaces[k].shape) == 1\n",
        "            total += space.spaces[k].shape[0]\n",
        "        return total\n",
        "    else:\n",
        "        raise ValueError(\"Incorrectly formatted observation space\")\n",
        "\n",
        "# Again, just like last time we have provided a suggested format.\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, layers=[128, 128], act='ReLU'):\n",
        "        ... # YOUR CODE HERE\n",
        "        # Implement a multi-layer perceptron using Linear layers and act activations\n",
        "        # torch Sequential could be useful here\n",
        "\n",
        "    def forward(self, x):\n",
        "        ... # YOUR CODE HERE\n",
        "        # Implement NLP forward pass\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, observation_space, action_space, layers=[128, 128], act='Tanh'):\n",
        "        ... # YOUR CODE HERE\n",
        "        # Initialize an Actor network\n",
        "        # An MLP should be enough!\n",
        "\n",
        "        # Extra Consideration: We want to ensure that the output matches the action space!\n",
        "        # Hint: Consider action_space.low and action_space.high\n",
        "        #       and how you would normalize and unnormalize something: subtract mean, divide by width factor\n",
        "        #       or for scaling back up, multiply by width factor, add mean.\n",
        "\n",
        "    def forward(self, state):\n",
        "        ... # YOUR CODE HERE\n",
        "        # Remember to choose a final activation that allows your actor\n",
        "        # to output values in the range that you want!\n",
        "\n",
        "        # The Pendulum environment source code is at\n",
        "        # https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, observation_space, action_space, layers=[128, 128], act='Tanh'):\n",
        "        ... # YOUR CODE HERE\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        ... # YOUR CODE HERE\n",
        "        # To pass both state and action to the critic,\n",
        "        # you can just concatenate them together."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo7l8AAjzeHn"
      },
      "source": [
        "## b) Replay Buffer\n",
        "In off-policy RL, we can re-use all our existing experience. We use a replay buffer to do this.\n",
        "\n",
        "A replay buffer is essentially a LIFO queue for a fixed amount of data. As soon as the buffer is full, we eject the least recent sample to make room for a new one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y59hp3p2zoGQ"
      },
      "source": [
        "from typing import NamedTuple\n",
        "import random\n",
        "import copy\n",
        "\n",
        "\n",
        "class Batch(NamedTuple):\n",
        "    obs: torch.Tensor\n",
        "    ac: torch.Tensor\n",
        "    next_obs: torch.Tensor\n",
        "    done: torch.Tensor\n",
        "    rew: torch.Tensor\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    \n",
        "    def __init__(self, env, observation_space, action_space, size):\n",
        "        self.obs = np.zeros((size, space_size(observation_space)), dtype=np.float32)\n",
        "        self.next_obs = np.zeros((size, space_size(observation_space)), dtype=np.float32)\n",
        "        self.ac = np.zeros((size, space_size(action_space)), dtype=np.float32)\n",
        "        self.rew = np.zeros(size, dtype=np.float32)\n",
        "        self.done = np.zeros(size, dtype=np.float32)\n",
        "        self.idx = 0\n",
        "        self.current_size = 0\n",
        "        self.env = env\n",
        "\n",
        "    def add(self, obs, ac, next_obs, rew, done):\n",
        "        # YOUR CODE HERE: Add the current information to position self.idx in the buffer\n",
        "       \n",
        "\n",
        "        # YOUR CODE HERE: Update self.idx to the position of the next data point to be added\n",
        "        # Hint: Remember that the buffer can fill up!\n",
        "        self.idx = ...\n",
        "\n",
        "        # Keep track of the current size of the replace buffer so it can be appropriately sampled from!\n",
        "        self.current_size = ... # YOUR CODE HERE\n",
        "            \n",
        "    def sample(self, batch_size):\n",
        "        # get random permutation\n",
        "        idxs = np.random.randint(0, self.current_size, size=batch_size)\n",
        "        return Batch(*map(torch.tensor, (self.obs[idxs], self.ac[idxs], self.next_obs[idxs], self.done[idxs], self.rew[idxs])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvhCqPoD0BDE"
      },
      "source": [
        "## c) Training Class\n",
        "For better coding practice, we'll define the algorithm in a class this time.\n",
        "\n",
        "You will need to be sure to do the following:\n",
        "1. Create the replay buffer\n",
        "2. Create the actor and critic\n",
        "3. Create the target actor and target critic as EMA (exponential moving average) of the actor and critic.\n",
        "4. Compute the Q loss and the actor loss appropriately\n",
        "5. Add noise to the actions. The amount of noise you add is a hyper-parameter that could need to be tuned.\n",
        "6. Define the training loop.\n",
        "7. For extra speed make sure you disable- and re-enable gradients on models you aren't updating!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKKY4BNg0HCH"
      },
      "source": [
        "class DDPG(object):\n",
        "\n",
        "    def __init__(self, env, update_freq=4, lr=0.001, batch_size=128, buffer_size=500000, layers=[128, 128], act='Tanh'):\n",
        "        # Params\n",
        "        self.gamma = 0.98\n",
        "        self.tau = 0.995\n",
        "        self.env = env\n",
        "        self.update_freq = update_freq\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Setup\n",
        "        # YOUR CODE HERE\n",
        "        # Create the replay buffer, actor, target actor, critic, and target critic\n",
        "        ...\n",
        "        # Copy the critic weights to the target critic, and the actor weights to the target actor\n",
        "        # You can get the weights of a network with\n",
        "        # [network].state_dict()\n",
        "        # and load them with [network].load_state_dict(...)\n",
        "        ...\n",
        "        # Freeze the gradients of the target actor and target critic\n",
        "        # You can iterate over the weights with [network].parameters()\n",
        "        # and freeze them by setting .requires_grad to False \n",
        "        ...\n",
        "\n",
        "        # Initialize Adam optimizers for the actor and critic\n",
        "        ...\n",
        "\n",
        "    def update_critic(self, batch):\n",
        "        # Don't forget to zero gradients!\n",
        "        ...\n",
        "        # TODO: compute the target\n",
        "        # The formula for the target reward is\n",
        "        # R + ð² Q'(s, a)\n",
        "        # where ð² is the discount factor, and Q' is the frozen (target) critic.\n",
        "        # Note that if the batch is final (check batch.done)\n",
        "        # you should not include the target critic's estimate,\n",
        "        # since we know the future reward must be 0.\n",
        "        with torch.no_grad():\n",
        "            target = ...\n",
        "        pred = ... # TODO: Compute prediction with the critic\n",
        "        # Just a sanity check\n",
        "        assert target.shape == pred.shape\n",
        "        # TODO: Compute loss, backprop, optimize. Use MSE loss\n",
        "        ...\n",
        "        # TODO: Return loss as a scalar, for printing statistics, etc.\n",
        "        # Remember, x.item() gets the value of a one-element tensor.\n",
        "        ...\n",
        "\n",
        "    def update_actor(self, batch):\n",
        "        # TODO: Freeze the critic gradients!\n",
        "        # Otherwise the critic would also train here,\n",
        "        # which would horribly break the model.\n",
        "        ...\n",
        "        # TODO: What do you need to do to the optimizer's gradients here?\n",
        "        # (you also had to do this for the critic update)\n",
        "        ...\n",
        "        # TODO: Compute the predicted action & its Q value.\n",
        "        # Since we want to maximize the Q value but minimize loss,\n",
        "        # what change do yo need to make to the Q value\n",
        "        # to get a loss?\n",
        "        ...\n",
        "        # TODO: Backprop & optimize\n",
        "        ...\n",
        "        # Re-enable gradients for the critic network\n",
        "        ...\n",
        "        # TODO: Return the loss as a scalar\n",
        "        ...\n",
        "\n",
        "    def learn(self, timesteps, log_freq=100):\n",
        "        num_updates = 0\n",
        "        ep_rewards = []\n",
        "        ep_lengths = []\n",
        "        rew_list = []\n",
        "        obs = env.reset()\n",
        "        \n",
        "        for i in range(timesteps):\n",
        "\n",
        "            if i % self.update_freq == 0 and len(ep_rewards) > 10:\n",
        "                # TODO: Update critic and actor with a sample from the replay buffer.\n",
        "                batch = ... # Hint: this is what the replay buffer is for.\n",
        "                critic_loss = ... # Hint: you should have methods for these.\n",
        "                actor_loss = ...\n",
        "\n",
        "                # Polyak average the target network\n",
        "                # This is some fancy in-place operation that makes the target networks\n",
        "                # The exponential moving average of the real networks.\n",
        "                with torch.no_grad():\n",
        "                    for p, target_p in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "                        target_p.data.mul_(self.tau)\n",
        "                        target_p.data.add_((1 - self.tau) * p.data)\n",
        "                    for p, target_p in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "                        target_p.data.mul_(self.tau)\n",
        "                        target_p.data.add_((1 - self.tau) * p.data)\n",
        "\n",
        "                num_updates += 1\n",
        "                if num_updates % log_freq == 0:\n",
        "                    print(\"===========================\")\n",
        "                    print(\"Timesteps:\", i, \"Updates:\", num_updates)\n",
        "                    print(\"Actor Loss\", actor_loss)\n",
        "                    print(\"Critic Loss\", critic_loss)\n",
        "                    print(\"10 Ep Avg Rew\", np.mean(ep_rewards[-10:]))\n",
        "\n",
        "            # Take a step in the environment\n",
        "            if len(ep_rewards) > 10:\n",
        "                with torch.no_grad():\n",
        "                    obs = torch.tensor(obs).unsqueeze(0).float()\n",
        "                    ac = self.actor(obs)[0].cpu().numpy()\n",
        "                    ac += 0.05 * np.random.randn(self.env.action_space.shape[0]) # Add action noise.\n",
        "                    ac = np.clip(ac, self.env.action_space.low , self.env.action_space.high)\n",
        "            else:\n",
        "                # Initially fill the buffer with some random actions.\n",
        "                ac = self.env.action_space.sample()\n",
        "\n",
        "            # TODO: Step based on the action, and add an (s, a, s', r, done)\n",
        "            # tuple to the replay buffer.\n",
        "            # Remember, \"s\" is referred to as \"obs\" in gym-speak.\n",
        "            # Also, update the rew_list with the reward,\n",
        "            # and update the current observation to s'.\n",
        "            ...\n",
        "\n",
        "            if done:\n",
        "                ep_rewards.append(sum(rew_list))\n",
        "                ep_lengths.append(len(rew_list))\n",
        "                rew_list = []\n",
        "                # TODO: Reset the gym environment\n",
        "                ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gb9BaVA0ybh"
      },
      "source": [
        "## d) Testing!\n",
        "We'll test on the pendulum environment.\n",
        "The code for this should be super simple!\n",
        "Try to get the best final reward possible. Warning: this could take upwards of 30 minutes to run. However, you should see it improve pretty rapidly. Unlike REINFORCE, DDPG is a much more stable algorithm :)\n",
        "\n",
        "Within 30000 timesteps and 7000 updates I was able to get to a reward of around -300\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ewNGILD088R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67c21b03-5311-4d07-8657-6f3246b6605f"
      },
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "alg = DDPG(env, batch_size=128, update_freq=4, lr=0.001)\n",
        "alg.learn(500000, log_freq=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "===========================\n",
            "Timesteps: 6196 Updates: 1000\n",
            "Actor Loss 35.6163215637207\n",
            "Critic Loss 1.0104421377182007\n",
            "10 Ep Avg Rew -1552.6277272023556\n",
            "===========================\n",
            "Timesteps: 10196 Updates: 2000\n",
            "Actor Loss 62.32590103149414\n",
            "Critic Loss 22.844600677490234\n",
            "10 Ep Avg Rew -1253.9500438137002\n",
            "===========================\n",
            "Timesteps: 14196 Updates: 3000\n",
            "Actor Loss 74.58618927001953\n",
            "Critic Loss 135.48696899414062\n",
            "10 Ep Avg Rew -641.433891980112\n",
            "===========================\n",
            "Timesteps: 18196 Updates: 4000\n",
            "Actor Loss 83.35975646972656\n",
            "Critic Loss 1.9298548698425293\n",
            "10 Ep Avg Rew -585.2419054470022\n",
            "===========================\n",
            "Timesteps: 22196 Updates: 5000\n",
            "Actor Loss 77.32152557373047\n",
            "Critic Loss 2.345503330230713\n",
            "10 Ep Avg Rew -344.52475059043246\n",
            "===========================\n",
            "Timesteps: 26196 Updates: 6000\n",
            "Actor Loss 79.16120910644531\n",
            "Critic Loss 2.923503875732422\n",
            "10 Ep Avg Rew -322.01495178442383\n",
            "===========================\n",
            "Timesteps: 30196 Updates: 7000\n",
            "Actor Loss 79.00096130371094\n",
            "Critic Loss 2.4271299839019775\n",
            "10 Ep Avg Rew -225.34763000011353\n",
            "===========================\n",
            "Timesteps: 34196 Updates: 8000\n",
            "Actor Loss 69.19931030273438\n",
            "Critic Loss 3.3824262619018555\n",
            "10 Ep Avg Rew -319.51315243196393\n",
            "===========================\n",
            "Timesteps: 38196 Updates: 9000\n",
            "Actor Loss 56.15282440185547\n",
            "Critic Loss 4.098389148712158\n",
            "10 Ep Avg Rew -233.46668590084488\n",
            "===========================\n",
            "Timesteps: 42196 Updates: 10000\n",
            "Actor Loss 72.42645263671875\n",
            "Critic Loss 3.0999624729156494\n",
            "10 Ep Avg Rew -405.39683555996425\n",
            "===========================\n",
            "Timesteps: 46196 Updates: 11000\n",
            "Actor Loss 56.4022216796875\n",
            "Critic Loss 133.0911407470703\n",
            "10 Ep Avg Rew -432.0523111107403\n",
            "===========================\n",
            "Timesteps: 50196 Updates: 12000\n",
            "Actor Loss 67.52189636230469\n",
            "Critic Loss 6.689306259155273\n",
            "10 Ep Avg Rew -354.59263437763815\n",
            "===========================\n",
            "Timesteps: 54196 Updates: 13000\n",
            "Actor Loss 59.014888763427734\n",
            "Critic Loss 64.07047271728516\n",
            "10 Ep Avg Rew -243.00570441626147\n",
            "===========================\n",
            "Timesteps: 58196 Updates: 14000\n",
            "Actor Loss 43.844032287597656\n",
            "Critic Loss 3.4322915077209473\n",
            "10 Ep Avg Rew -254.35375377554874\n",
            "===========================\n",
            "Timesteps: 62196 Updates: 15000\n",
            "Actor Loss 39.486968994140625\n",
            "Critic Loss 3.8291637897491455\n",
            "10 Ep Avg Rew -292.9919597649363\n",
            "===========================\n",
            "Timesteps: 66196 Updates: 16000\n",
            "Actor Loss 33.634498596191406\n",
            "Critic Loss 2.005298137664795\n",
            "10 Ep Avg Rew -252.02317736748068\n",
            "===========================\n",
            "Timesteps: 70196 Updates: 17000\n",
            "Actor Loss 48.082733154296875\n",
            "Critic Loss 4.8366522789001465\n",
            "10 Ep Avg Rew -253.2066104843563\n",
            "===========================\n",
            "Timesteps: 74196 Updates: 18000\n",
            "Actor Loss 45.38853454589844\n",
            "Critic Loss 2.821779251098633\n",
            "10 Ep Avg Rew -214.4927841035073\n",
            "===========================\n",
            "Timesteps: 78196 Updates: 19000\n",
            "Actor Loss 39.66761779785156\n",
            "Critic Loss 263.6838684082031\n",
            "10 Ep Avg Rew -258.04165052737346\n",
            "===========================\n",
            "Timesteps: 82196 Updates: 20000\n",
            "Actor Loss 43.59661865234375\n",
            "Critic Loss 3.1490883827209473\n",
            "10 Ep Avg Rew -282.1078505878629\n",
            "===========================\n",
            "Timesteps: 86196 Updates: 21000\n",
            "Actor Loss 40.938812255859375\n",
            "Critic Loss 1.7788736820220947\n",
            "10 Ep Avg Rew -264.5715124141074\n",
            "===========================\n",
            "Timesteps: 90196 Updates: 22000\n",
            "Actor Loss 41.09226989746094\n",
            "Critic Loss 1.163881540298462\n",
            "10 Ep Avg Rew -263.16494347059404\n",
            "===========================\n",
            "Timesteps: 94196 Updates: 23000\n",
            "Actor Loss 29.321134567260742\n",
            "Critic Loss 4.149723052978516\n",
            "10 Ep Avg Rew -209.89787026626618\n",
            "===========================\n",
            "Timesteps: 98196 Updates: 24000\n",
            "Actor Loss 32.21997833251953\n",
            "Critic Loss 33.5763053894043\n",
            "10 Ep Avg Rew -220.26948858437927\n",
            "===========================\n",
            "Timesteps: 102196 Updates: 25000\n",
            "Actor Loss 43.25025177001953\n",
            "Critic Loss 2.179715633392334\n",
            "10 Ep Avg Rew -277.8825267019005\n",
            "===========================\n",
            "Timesteps: 106196 Updates: 26000\n",
            "Actor Loss 33.09467315673828\n",
            "Critic Loss 1.3521181344985962\n",
            "10 Ep Avg Rew -204.96479959553432\n",
            "===========================\n",
            "Timesteps: 110196 Updates: 27000\n",
            "Actor Loss 29.869876861572266\n",
            "Critic Loss 5.5959272384643555\n",
            "10 Ep Avg Rew -289.79961875741714\n",
            "===========================\n",
            "Timesteps: 114196 Updates: 28000\n",
            "Actor Loss 33.31566619873047\n",
            "Critic Loss 3.2735977172851562\n",
            "10 Ep Avg Rew -241.57754763187253\n",
            "===========================\n",
            "Timesteps: 118196 Updates: 29000\n",
            "Actor Loss 28.708959579467773\n",
            "Critic Loss 3.4982776641845703\n",
            "10 Ep Avg Rew -260.7551860850815\n",
            "===========================\n",
            "Timesteps: 122196 Updates: 30000\n",
            "Actor Loss 36.39913558959961\n",
            "Critic Loss 2.7400803565979004\n",
            "10 Ep Avg Rew -250.05960171971515\n",
            "===========================\n",
            "Timesteps: 126196 Updates: 31000\n",
            "Actor Loss 32.482566833496094\n",
            "Critic Loss 4.8534698486328125\n",
            "10 Ep Avg Rew -235.86993276639464\n",
            "===========================\n",
            "Timesteps: 130196 Updates: 32000\n",
            "Actor Loss 15.87716293334961\n",
            "Critic Loss 2.914426803588867\n",
            "10 Ep Avg Rew -278.36060943513496\n",
            "===========================\n",
            "Timesteps: 134196 Updates: 33000\n",
            "Actor Loss 15.66256332397461\n",
            "Critic Loss 0.9567211866378784\n",
            "10 Ep Avg Rew -219.98653370789347\n",
            "===========================\n",
            "Timesteps: 138196 Updates: 34000\n",
            "Actor Loss 25.26318359375\n",
            "Critic Loss 1.8582438230514526\n",
            "10 Ep Avg Rew -232.94393816768974\n",
            "===========================\n",
            "Timesteps: 142196 Updates: 35000\n",
            "Actor Loss 26.578439712524414\n",
            "Critic Loss 2.8176469802856445\n",
            "10 Ep Avg Rew -304.13075545958986\n",
            "===========================\n",
            "Timesteps: 146196 Updates: 36000\n",
            "Actor Loss 33.736961364746094\n",
            "Critic Loss 1.1916484832763672\n",
            "10 Ep Avg Rew -196.15112104961662\n",
            "===========================\n",
            "Timesteps: 150196 Updates: 37000\n",
            "Actor Loss 30.433504104614258\n",
            "Critic Loss 2.5277907848358154\n",
            "10 Ep Avg Rew -157.7489148504075\n",
            "===========================\n",
            "Timesteps: 154196 Updates: 38000\n",
            "Actor Loss 35.05265426635742\n",
            "Critic Loss 1.6279722452163696\n",
            "10 Ep Avg Rew -207.44807941156014\n",
            "===========================\n",
            "Timesteps: 158196 Updates: 39000\n",
            "Actor Loss 28.275651931762695\n",
            "Critic Loss 2.0794405937194824\n",
            "10 Ep Avg Rew -287.9906437221237\n",
            "===========================\n",
            "Timesteps: 162196 Updates: 40000\n",
            "Actor Loss 38.832115173339844\n",
            "Critic Loss 1.3717304468154907\n",
            "10 Ep Avg Rew -310.65015076191526\n",
            "===========================\n",
            "Timesteps: 166196 Updates: 41000\n",
            "Actor Loss 30.356101989746094\n",
            "Critic Loss 2.5539193153381348\n",
            "10 Ep Avg Rew -252.99521970614305\n",
            "===========================\n",
            "Timesteps: 170196 Updates: 42000\n",
            "Actor Loss 22.63149070739746\n",
            "Critic Loss 2.8089404106140137\n",
            "10 Ep Avg Rew -206.22459119157148\n",
            "===========================\n",
            "Timesteps: 174196 Updates: 43000\n",
            "Actor Loss 32.17279815673828\n",
            "Critic Loss 0.6564738750457764\n",
            "10 Ep Avg Rew -244.00625148894846\n",
            "===========================\n",
            "Timesteps: 178196 Updates: 44000\n",
            "Actor Loss 25.660085678100586\n",
            "Critic Loss 3.900411367416382\n",
            "10 Ep Avg Rew -281.096526313519\n",
            "===========================\n",
            "Timesteps: 182196 Updates: 45000\n",
            "Actor Loss 31.102323532104492\n",
            "Critic Loss 0.7350730299949646\n",
            "10 Ep Avg Rew -258.80831797301124\n",
            "===========================\n",
            "Timesteps: 186196 Updates: 46000\n",
            "Actor Loss 37.866878509521484\n",
            "Critic Loss 2.039281129837036\n",
            "10 Ep Avg Rew -241.97736293059597\n",
            "===========================\n",
            "Timesteps: 190196 Updates: 47000\n",
            "Actor Loss 38.07917404174805\n",
            "Critic Loss 1.6361808776855469\n",
            "10 Ep Avg Rew -246.05463600856274\n",
            "===========================\n",
            "Timesteps: 194196 Updates: 48000\n",
            "Actor Loss 20.102493286132812\n",
            "Critic Loss 1.1295292377471924\n",
            "10 Ep Avg Rew -291.12592624684675\n",
            "===========================\n",
            "Timesteps: 198196 Updates: 49000\n",
            "Actor Loss 31.38199234008789\n",
            "Critic Loss 1.974312663078308\n",
            "10 Ep Avg Rew -337.56511346723534\n",
            "===========================\n",
            "Timesteps: 202196 Updates: 50000\n",
            "Actor Loss 31.131649017333984\n",
            "Critic Loss 2.017695665359497\n",
            "10 Ep Avg Rew -245.12364806699466\n",
            "===========================\n",
            "Timesteps: 206196 Updates: 51000\n",
            "Actor Loss 26.706974029541016\n",
            "Critic Loss 1.1343616247177124\n",
            "10 Ep Avg Rew -309.5800460910207\n",
            "===========================\n",
            "Timesteps: 210196 Updates: 52000\n",
            "Actor Loss 30.65285873413086\n",
            "Critic Loss 1.105485439300537\n",
            "10 Ep Avg Rew -365.88003282401223\n",
            "===========================\n",
            "Timesteps: 214196 Updates: 53000\n",
            "Actor Loss 38.6771354675293\n",
            "Critic Loss 2.6519739627838135\n",
            "10 Ep Avg Rew -284.3064306542306\n",
            "===========================\n",
            "Timesteps: 218196 Updates: 54000\n",
            "Actor Loss 22.302244186401367\n",
            "Critic Loss 1.507685899734497\n",
            "10 Ep Avg Rew -317.6735400281895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9078e8202282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(env.action_space.low, env.action_space.high)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0malg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-79dedc2e3e41>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, timesteps, log_freq)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mactor_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-79dedc2e3e41>\u001b[0m in \u001b[0;36mupdate_critic\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqGin7Id1DNS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}