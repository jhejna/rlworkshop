{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MF_RL_Workshop_Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhejna/rlworkshop/blob/main/MF_RL_Workshop_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TofUv8vEed"
      },
      "source": [
        "# 0. Setup\n",
        "Run the following cells to test the installation and make sure everything is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNMcYrJUwwdt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702914d6-eb64-497a-feb0-ae08daa4a6da"
      },
      "source": [
        "# Check if we have GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU working!\")\n",
        "else:\n",
        "    print(\"WARNING: The Colab runtime does not have GPU enabled.\")\n",
        "    print(\"         You should be able to train the models on CPU, but if you\")\n",
        "    print(\"         want GPU, enable it in Runtime > Change Runtime Type\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: The Colab runtime does not have GPU enabled.\n",
            "         You should be able to train the models on CPU, but if you\n",
            "         want GPU, enable it in Runtime > Change Runtime Type\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKmwVkibl2ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e08d1e-84da-4504-838c-17983191dc10"
      },
      "source": [
        "%%bash\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,977 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n",
            "Fetched 993 kB in 1s (1,366 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144793 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\r\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Selecting previously unselected package x11-utils.\r\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\r\n",
            "Unpacking x11-utils (7.7+3build1) ...\r\n",
            "Selecting previously unselected package xvfb.\r\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\r\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\r\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\r\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n",
            "Setting up x11-utils (7.7+3build1) ...\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cttFN8FfrpV0"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(640, 480))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6QcBFWArfxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44560eb2-c327-4960-92e4-c0687d8676a4"
      },
      "source": [
        "# This should produce a non-blank output.\n",
        "!echo $DISPLAY"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ":0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5tV_P3Pl9eh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c2acc6f1-02e2-413c-fea0-82544ae28e75"
      },
      "source": [
        "# Test rendering a gym environment\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "num_ep = 0\n",
        "obs = env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "for _ in range(100):\n",
        "    ac = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(ac)\n",
        "    img.set_data(env.render(mode='rgb_array')) \n",
        "    plt.axis('off')\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        num_ep += 1\n",
        "env.close()\n",
        "del env\n",
        "del img\n",
        "print(\"Episodes:\", num_ep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUmNsgmMvtla"
      },
      "source": [
        "# 1. REINFORCE\n",
        "Here we'll implement the basic policy gradient algorithm REINFORCE in pytorch for the above cart-pole environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V80xIEMvuxz"
      },
      "source": [
        "# Import the required modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYtOXM5in-cr"
      },
      "source": [
        "## a) Policy Network\n",
        "First we need to implement the policy network. The policy network takes in observations and outputs actions that should be taken. Our policy will be implemented in the form of an MLP\n",
        "\n",
        "Our policy class must do the following:\n",
        "1. Take in an observation from the environment, and return the action to take.\n",
        "2. Given an observation and action, compute the log probability of taking that action under the current policy. This is used for computing the REINFORCE loss.\n",
        "\n",
        "There are a few considerations to keep in mind:\n",
        "\n",
        "**Discrete Actions**: The cartpole environment has an action space of size 2, indicating if the cart should move to the left or right. This means we will need to output a categorical distribution. Think about the best way to implement this. (Hint: classification)\n",
        "\n",
        "**Exploration**: Remember that for RL to work, we need to explore. Make sure you have some randomness in your action selection.\n",
        "\n",
        "Fill out the following class to complete the policy network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka7e7SzZwExu"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, layers=[128, 128], act=nn.ReLU):\n",
        "        super().__init__()\n",
        "        last_dim = in_dim\n",
        "        model = []\n",
        "        for layer in layers:\n",
        "            model.append(nn.Linear(last_dim, layer))\n",
        "            last_dim = layer\n",
        "            model.append(act())\n",
        "        model.append(nn.Linear(last_dim, out_dim))\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, action_space, observation_space, layers=[64, 64], act=nn.ReLU):\n",
        "        super().__init__()\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = observation_space\n",
        "        self.net = MLP(observation_space.shape[0], action_space.n, layers=layers, act=act)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        logits = self.net(obs)\n",
        "        action_distribution = F.softmax(logits)\n",
        "        return action_distribution\n",
        "\n",
        "    def get_action(self, obs, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
        "            action_distribution = self(obs)[0].cpu().numpy()\n",
        "        if deterministic:\n",
        "            return np.argmax(action_distribution)\n",
        "        else:\n",
        "            # print(self.action_space.n, action_distribution, np.random.choice(self.action_space.n, p=action_distribution))\n",
        "            return np.random.choice(self.action_space.n, p=action_distribution)\n",
        "        \n",
        "    def log_prob(self, action, obs):\n",
        "        assert len(obs.shape) == 2, \"Observation must be batched for log_prob\"\n",
        "        action_distribution = self(obs)\n",
        "        probs = torch.gather(action_distribution, -1, action)\n",
        "        return torch.log(probs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj7X8HkrxyOU"
      },
      "source": [
        "## b) Training Loop\n",
        "Next, we need to actually train the policy network. This is done in two phases:\n",
        "1. Data Collection: execute the policy in the enviornment and explore, collecting experience. For this you may need to refer to the gym environment interface. See how we did it in the setup or check out the documentation here: https://gym.openai.com/docs/\n",
        "2. Update the policy: use the collected experience to update the policy using the computed policy gradient.\n",
        "\n",
        "We have provided some skeleton code for the training loop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvV0Zj0NxzZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338d131b-a5e2-4286-89c5-bb7e602f1049"
      },
      "source": [
        "# Hyper-parameters\n",
        "iterations = 64\n",
        "epochs_per_iter = 4\n",
        "timesteps_per_iter = 2048\n",
        "batch_size = 512\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "obs = env.reset()\n",
        "\n",
        "policy = Policy(env.action_space, env.observation_space, layers=[64, 64], act=nn.Tanh)\n",
        "optim = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(iterations):\n",
        "    # First, collect data\n",
        "    \n",
        "    # Setup the data collection buffers\n",
        "    observations = np.zeros((timesteps_per_iter, *env.observation_space.shape), dtype=np.float)\n",
        "    actions = np.zeros((timesteps_per_iter, 1), np.float)\n",
        "    rewards = np.zeros(timesteps_per_iter, np.float)\n",
        "    dones = np.zeros(timesteps_per_iter, dtype=np.bool)\n",
        "    \n",
        "    # Collect the data\n",
        "    for i in range(timesteps_per_iter):\n",
        "        action = policy.get_action(obs, deterministic=False)\n",
        "        # Record the observations and action\n",
        "        observations[i], actions[i] = obs, action\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        # Record the consequences\n",
        "        rewards[i], dones[i] = reward, done\n",
        "        if done:\n",
        "            obs = env.reset()\n",
        "    print(\"Average Reward:\", np.sum(rewards) / np.sum(dones))\n",
        "\n",
        "    # Compute the reward to go\n",
        "    reward_to_go = np.zeros(timesteps_per_iter)\n",
        "    reward_to_go[-1] = rewards[-1] # init the reward to go array\n",
        "\n",
        "    for i in reversed(range(timesteps_per_iter - 1)):\n",
        "        reward_to_go[i] = rewards[i] + gamma * reward_to_go[i+1] * (1 - dones[i+1])\n",
        "    \n",
        "    # Compute the advantage\n",
        "    advantages = reward_to_go - np.mean(reward_to_go)\n",
        "\n",
        "    # Convert everything to pytorch objects now for efficiency\n",
        "    observations = torch.from_numpy(observations).float()\n",
        "    actions = torch.from_numpy(actions).long()\n",
        "    advantages = torch.from_numpy(advantages).float()\n",
        "    \n",
        "    # Update the policy\n",
        "    for epoch in range(epochs_per_iter):\n",
        "        indices = np.random.permutation(timesteps_per_iter)\n",
        "        # Note this is a bit sloppy as we don't necesarily hit all data points if batch does not evenly divide timesteps \n",
        "        for i in range(timesteps_per_iter // batch_size): \n",
        "            batch_indices = indices[i*batch_size:(i+1)*batch_size]\n",
        "            batch_obs, batch_actions, batch_advantages = observations[batch_indices], actions[batch_indices], advantages[batch_indices]\n",
        "            # Now update the policy\n",
        "            optim.zero_grad()            \n",
        "            log_probs = policy.log_prob(batch_actions, batch_obs).squeeze()\n",
        "            assert len(log_probs.shape) == 1\n",
        "            prob_weighted_rewards = batch_advantages * log_probs\n",
        "            loss = -torch.mean(prob_weighted_rewards)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "    print(\"Policy Loss:\", loss.item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average Reward: 22.021505376344088\n",
            "Policy Loss: -0.22406628727912903\n",
            "Average Reward: 27.30666666666667\n",
            "Policy Loss: -0.10433503240346909\n",
            "Average Reward: 31.50769230769231\n",
            "Policy Loss: -0.6832255125045776\n",
            "Average Reward: 30.11764705882353\n",
            "Policy Loss: -0.4259313941001892\n",
            "Average Reward: 37.925925925925924\n",
            "Policy Loss: 0.34014275670051575\n",
            "Average Reward: 45.51111111111111\n",
            "Policy Loss: -0.4217386543750763\n",
            "Average Reward: 38.64150943396226\n",
            "Policy Loss: -1.1334425210952759\n",
            "Average Reward: 43.57446808510638\n",
            "Policy Loss: 1.18501615524292\n",
            "Average Reward: 42.666666666666664\n",
            "Policy Loss: -0.6878674030303955\n",
            "Average Reward: 51.2\n",
            "Policy Loss: -1.5754674673080444\n",
            "Average Reward: 43.57446808510638\n",
            "Policy Loss: -1.2896825075149536\n",
            "Average Reward: 62.06060606060606\n",
            "Policy Loss: -0.17201776802539825\n",
            "Average Reward: 66.06451612903226\n",
            "Policy Loss: -0.5626804232597351\n",
            "Average Reward: 55.351351351351354\n",
            "Policy Loss: -1.4731146097183228\n",
            "Average Reward: 49.951219512195124\n",
            "Policy Loss: -0.5225249528884888\n",
            "Average Reward: 73.14285714285714\n",
            "Policy Loss: 0.3103905916213989\n",
            "Average Reward: 66.06451612903226\n",
            "Policy Loss: -0.6855745315551758\n",
            "Average Reward: 73.14285714285714\n",
            "Policy Loss: 1.0138683319091797\n",
            "Average Reward: 78.76923076923077\n",
            "Policy Loss: -0.21211707592010498\n",
            "Average Reward: 78.76923076923077\n",
            "Policy Loss: -2.487623453140259\n",
            "Average Reward: 81.92\n",
            "Policy Loss: -2.043468713760376\n",
            "Average Reward: 113.77777777777777\n",
            "Policy Loss: -5.453023433685303\n",
            "Average Reward: 93.0909090909091\n",
            "Policy Loss: -3.692850112915039\n",
            "Average Reward: 113.77777777777777\n",
            "Policy Loss: -1.3644970655441284\n",
            "Average Reward: 113.77777777777777\n",
            "Policy Loss: -3.471700429916382\n",
            "Average Reward: 102.4\n",
            "Policy Loss: -1.4378594160079956\n",
            "Average Reward: 146.28571428571428\n",
            "Policy Loss: -2.4044923782348633\n",
            "Average Reward: 136.53333333333333\n",
            "Policy Loss: -1.975900650024414\n",
            "Average Reward: 186.1818181818182\n",
            "Policy Loss: -0.32958805561065674\n",
            "Average Reward: 186.1818181818182\n",
            "Policy Loss: -1.0079598426818848\n",
            "Average Reward: 146.28571428571428\n",
            "Policy Loss: 0.9608814120292664\n",
            "Average Reward: 170.66666666666666\n",
            "Policy Loss: -5.182910442352295\n",
            "Average Reward: 157.53846153846155\n",
            "Policy Loss: 1.8855211734771729\n",
            "Average Reward: 186.1818181818182\n",
            "Policy Loss: -2.698514938354492\n",
            "Average Reward: 292.57142857142856\n",
            "Policy Loss: -2.9395103454589844\n",
            "Average Reward: 204.8\n",
            "Policy Loss: 3.8531064987182617\n",
            "Average Reward: 204.8\n",
            "Policy Loss: 1.301332950592041\n",
            "Average Reward: 256.0\n",
            "Policy Loss: -7.104659080505371\n",
            "Average Reward: 227.55555555555554\n",
            "Policy Loss: -5.141783714294434\n",
            "Average Reward: 204.8\n",
            "Policy Loss: -3.7749643325805664\n",
            "Average Reward: 186.1818181818182\n",
            "Policy Loss: -4.458988189697266\n",
            "Average Reward: 146.28571428571428\n",
            "Policy Loss: -0.4537551701068878\n",
            "Average Reward: 204.8\n",
            "Policy Loss: 2.2700417041778564\n",
            "Average Reward: 204.8\n",
            "Policy Loss: 2.0520694255828857\n",
            "Average Reward: 227.55555555555554\n",
            "Policy Loss: -1.525469422340393\n",
            "Average Reward: 256.0\n",
            "Policy Loss: 1.7326620817184448\n",
            "Average Reward: 512.0\n",
            "Policy Loss: -0.23492449522018433\n",
            "Average Reward: 292.57142857142856\n",
            "Policy Loss: -10.450621604919434\n",
            "Average Reward: 341.3333333333333\n",
            "Policy Loss: -3.3002805709838867\n",
            "Average Reward: 292.57142857142856\n",
            "Policy Loss: -0.7272916436195374\n",
            "Average Reward: 256.0\n",
            "Policy Loss: 5.963630676269531\n",
            "Average Reward: 409.6\n",
            "Policy Loss: 3.830810070037842\n",
            "Average Reward: 341.3333333333333\n",
            "Policy Loss: -1.9558699131011963\n",
            "Average Reward: 341.3333333333333\n",
            "Policy Loss: -3.093061685562134\n",
            "Average Reward: 512.0\n",
            "Policy Loss: -5.487614631652832\n",
            "Average Reward: 409.6\n",
            "Policy Loss: 5.007084846496582\n",
            "Average Reward: 341.3333333333333\n",
            "Policy Loss: -1.3194332122802734\n",
            "Average Reward: 409.6\n",
            "Policy Loss: -1.927340030670166\n",
            "Average Reward: 341.3333333333333\n",
            "Policy Loss: -4.344740390777588\n",
            "Average Reward: 256.0\n",
            "Policy Loss: -3.187617301940918\n",
            "Average Reward: 292.57142857142856\n",
            "Policy Loss: -3.5705578327178955\n",
            "Average Reward: 409.6\n",
            "Policy Loss: 2.8407037258148193\n",
            "Average Reward: 341.3333333333333\n",
            "Policy Loss: -3.901792049407959\n",
            "Average Reward: 292.57142857142856\n",
            "Policy Loss: -4.31429386138916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNd7kXi4kJon"
      },
      "source": [
        "## c) Testing!\n",
        "RL, especially regular REINFORCE, is very unstable and sensitive to hyper-parameters. Try tuning the above algorithm  by changing hyper-parameters until you can get an average final reward of > 500 (This is possible!).\n",
        "\n",
        "Once you have a functioning policy, execute it in the environment and watch the results!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRIWdI2ecq57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a06954c7-0dad-4587-d669-28db09a08ed1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "eval_eps = 5\n",
        "ep_rewards = []\n",
        "obs = env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "while eval_eps > 0:\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    ep_reward = 0\n",
        "    while not done:\n",
        "        aciton = policy.get_action(obs, deterministic=True)\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        # Render\n",
        "        img.set_data(env.render(mode='rgb_array')) \n",
        "        plt.axis('off')\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "    ep_rewards.append(ep_reward)\n",
        "    eval_eps -= 1\n",
        "\n",
        "print(\"Average Eval Reward\", sum(ep_rewards) / len(ep_rewards))\n",
        "env.close()\n",
        "del env\n",
        "del img\n",
        "print(\"Episodes:\", num_ep)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Eval Reward 9.4\n",
            "Episodes: 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WTW9B0Ep30d"
      },
      "source": [
        "# 2. DDPG\n",
        "We previously implemented REINFORCE, the basic policy gradient method. Now we'll get fancier and implement DDPG.\n",
        "\n",
        "DDPG, or deep-deterministic policy gradients is an off policy Q-learning method for continuous action space environments. The paper can be found here: https://arxiv.org/abs/1509.02971. \n",
        "\n",
        "Here's a good summary of DDPG: https://spinningup.openai.com/en/latest/algorithms/ddpg.html\n",
        "\n",
        "\n",
        "DDPG has the following basic ingredients:\n",
        "1. A Q function that predicts the reward of different actions. Input = action + observation\n",
        "2. A target Q function that is the EMA of the regular Q function for regression targets\n",
        "3. A policy network that tries to learn to propose actions that maximize the Q function. As we are in continuous space, we can't simply take the argmax of the Q values.\n",
        "\n",
        "The code will be slightly re-factored in comparison to last time. We'll be using the pendulum enviornment because it is continuous instead of discrete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTtGt3pY1BAp"
      },
      "source": [
        "# Import the required modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXemO56fzFMY"
      },
      "source": [
        "## a) Actor-Critic Networks\n",
        "The first step is to implement the actor and critic models. For convenience I have provided a `space_size` function that will tell you the size of a gym space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHij7xvylEKl"
      },
      "source": [
        "def space_size(space):\n",
        "    if isinstance(space, gym.spaces.Box):\n",
        "        assert len(space.shape) == 1\n",
        "        return space.shape[0]\n",
        "    elif isinstance(space, gym.spaces.Dict):\n",
        "        total = 0\n",
        "        for k, v in space.spaces.items():\n",
        "            assert len(space.spaces[k].shape) == 1\n",
        "            total += space.spaces[k].shape[0]\n",
        "        return total\n",
        "    else:\n",
        "        raise ValueError(\"Incorrectly formatted observation space\")\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, layers=[128, 128], act='ReLU'):\n",
        "        super().__init__()\n",
        "        last_dim = in_dim\n",
        "        model = []\n",
        "        act = vars(nn)[act]\n",
        "        for layer in layers:\n",
        "            model.append(nn.Linear(last_dim, layer))\n",
        "            last_dim = layer\n",
        "            model.append(act())\n",
        "        model.append(nn.Linear(last_dim, out_dim))\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, observation_space, action_space, layers=[128, 128], act='Tanh'):\n",
        "        super().__init__()\n",
        "        self.mlp = MLP(space_size(observation_space), space_size(action_space), layers=layers, act=act)\n",
        "        self.action_bias = (action_space.high + action_space.low) / 2\n",
        "        self.action_scale = (action_space.high - action_space.low) / 2 # Divide by 2 because want interval to be width 2 (-1 to 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.mlp(state)\n",
        "        x = F.tanh(x)\n",
        "        # appropriatly scale actions to action_space\n",
        "        x = self.action_scale * x + self.action_bias\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, observation_space, action_space, layers=[128, 128], act='Tanh'):\n",
        "        super().__init__()\n",
        "        self.mlp = MLP(space_size(observation_space) + space_size(action_space), 1, layers=layers, act=act)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat((state, action), dim=-1)\n",
        "        x = self.mlp(x)\n",
        "        return x.squeeze(-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo7l8AAjzeHn"
      },
      "source": [
        "## b) Replay Buffer\n",
        "In off-policy RL, we can re-use all our existing experience. We use a replay buffer to do this.\n",
        "\n",
        "A replay buffer is essentially a LIFO queue for a fixed amount of data. As soon as the buffer is full, we eject the least recent sample to make room for a new one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y59hp3p2zoGQ"
      },
      "source": [
        "from typing import NamedTuple\n",
        "import random\n",
        "import copy\n",
        "\n",
        "\n",
        "class Batch(NamedTuple):\n",
        "    obs: torch.Tensor\n",
        "    ac: torch.Tensor\n",
        "    next_obs: torch.Tensor\n",
        "    done: torch.Tensor\n",
        "    rew: torch.Tensor\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    \n",
        "    def __init__(self, observation_space, action_space, size):\n",
        "        self.obs = np.zeros((size, space_size(observation_space)), dtype=np.float32)\n",
        "        self.next_obs = np.zeros((size, space_size(observation_space)), dtype=np.float32)\n",
        "        self.ac = np.zeros((size, space_size(action_space)), dtype=np.float32)\n",
        "        self.rew = np.zeros(size, dtype=np.float32)\n",
        "        self.done = np.zeros(size, dtype=np.float32)\n",
        "        self.idx = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def add(self, obs, ac, next_obs, rew, done):\n",
        "        self.obs[self.idx] = obs\n",
        "        self.ac[self.idx] = ac\n",
        "        self.next_obs[self.idx] = next_obs\n",
        "        self.rew[self.idx] = rew\n",
        "        self.done[self.idx] = done\n",
        "        self.idx += 1\n",
        "        if self.idx >= len(self.obs):\n",
        "            self.idx = 0\n",
        "        self.size = min(self.size + 1, len(self.obs))\n",
        "            \n",
        "    def sample(self, batch_size):\n",
        "        # get random permutation\n",
        "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
        "        return Batch(*map(torch.tensor, (self.obs[idxs], self.ac[idxs], self.next_obs[idxs], self.done[idxs], self.rew[idxs])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvhCqPoD0BDE"
      },
      "source": [
        "## c) Training Class\n",
        "For better coding practice, we'll define the algorithm in a class this time.\n",
        "\n",
        "You will need to be sure to do the following:\n",
        "1. Create the replay buffer\n",
        "2. Create the actor and critic\n",
        "3. Create the target actor and target critic as EMA of the actor and critic.\n",
        "4. Compute the Q loss and the actor loss appropriately\n",
        "5. Add noise to the actions. The amount of noise you add is a hyper-parameter that could need to be tuned.\n",
        "6. Define the training loop.\n",
        "7. For extra speed make sure you disable- and re-enable gradients on models you aren't updating!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKKY4BNg0HCH"
      },
      "source": [
        "class DDPG(object):\n",
        "\n",
        "    def __init__(self, env, update_freq=4, lr=0.001, batch_size=128, buffer_size=500000, layers=[128, 128], act='Tanh'):\n",
        "        # Params\n",
        "        self.gamma = 0.98\n",
        "        self.tau = 0.995\n",
        "        self.env = env\n",
        "        self.update_freq = update_freq\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Setup\n",
        "        self.replay_buffer = ReplayBuffer(env, env.observation_space, env.action_space, buffer_size)\n",
        "        self.actor = Actor(env.observation_space, env.action_space, layers=layers, act=act)\n",
        "        self.target_actor = Actor(env.observation_space, env.action_space, layers=layers, act=act)\n",
        "        self.critic = Critic(env.observation_space, env.action_space, layers=layers, act=act)\n",
        "        self.target_critic = Critic(env.observation_space, env.action_space, layers=layers, act=act)\n",
        "        # Copy the critic weights to the target\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        for p in self.target_actor.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "        for p in self.target_critic.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
        "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
        "\n",
        "    def update_critic(self, batch):\n",
        "        self.critic_opt.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            target = batch.rew + self.gamma*(1 - batch.done)*self.target_critic(batch.next_obs, self.target_actor(batch.next_obs))\n",
        "        pred = self.critic(batch.obs, batch.ac)\n",
        "        assert target.shape == pred.shape\n",
        "        loss = F.mse_loss(pred, target)\n",
        "        loss.backward()\n",
        "        self.critic_opt.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def update_actor(self, batch):\n",
        "        # Don't compute gradients for Q\n",
        "        for p in self.critic.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.actor_opt.zero_grad()\n",
        "        pred_ac = self.actor(batch.obs)\n",
        "        pred_q = self.critic(batch.obs, pred_ac)\n",
        "        loss = -1 * torch.mean(pred_q)\n",
        "        loss.backward()\n",
        "        self.actor_opt.step()\n",
        "        # Re-enable grads\n",
        "        for p in self.critic.parameters():\n",
        "            p.requires_grad = True\n",
        "        return loss.item()\n",
        "\n",
        "    def learn(self, timesteps, log_freq=100):\n",
        "        num_updates = 0\n",
        "        ep_rewards = []\n",
        "        ep_lengths = []\n",
        "        rew_list = []\n",
        "        obs = env.reset()\n",
        "        \n",
        "        for i in range(timesteps):\n",
        "\n",
        "            if i % self.update_freq == 0 and len(ep_rewards) > 10:\n",
        "                batch = self.replay_buffer.sample(self.batch_size)\n",
        "                critic_loss = self.update_critic(batch)\n",
        "                actor_loss = self.update_actor(batch)\n",
        "\n",
        "                # Polyak average the target network\n",
        "                with torch.no_grad():\n",
        "                    for p, target_p in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
        "                        target_p.data.mul_(self.tau)\n",
        "                        target_p.data.add_((1 - self.tau) * p.data)\n",
        "                    for p, target_p in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
        "                        target_p.data.mul_(self.tau)\n",
        "                        target_p.data.add_((1 - self.tau) * p.data)\n",
        "\n",
        "                num_updates += 1\n",
        "                if num_updates % log_freq == 0:\n",
        "                    print(\"===========================\")\n",
        "                    print(\"Timesteps:\", i, \"Updates:\", num_updates)\n",
        "                    print(\"Actor Loss\", actor_loss)\n",
        "                    print(\"Critic Loss\", critic_loss)\n",
        "                    print(\"10 Ep Avg Rew\", np.mean(ep_rewards[-10:]))\n",
        "\n",
        "            # Take a step in the environment\n",
        "            if len(ep_rewards) > 10:\n",
        "                with torch.no_grad():\n",
        "                    obs = torch.tensor(obs).unsqueeze(0).float()\n",
        "                    ac = self.actor(obs)[0].cpu().numpy()\n",
        "                    ac += 0.05 * np.random.randn(self.env.action_space.shape[0]) # Add action noise.\n",
        "                    ac = np.clip(ac, self.env.action_space.low , self.env.action_space.high)\n",
        "            else:\n",
        "                ac = self.env.action_space.sample()\n",
        "\n",
        "            next_obs, rew, done, _ = env.step(ac)\n",
        "            self.replay_buffer.add(obs, ac, next_obs, rew, done)\n",
        "            obs = next_obs\n",
        "            rew_list.append(rew)\n",
        "\n",
        "            if done:\n",
        "                ep_rewards.append(sum(rew_list))\n",
        "                ep_lengths.append(len(rew_list))\n",
        "                rew_list = []\n",
        "                obs = env.reset()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gb9BaVA0ybh"
      },
      "source": [
        "## d) Testing!\n",
        "We'll test on the pendulum environment.\n",
        "The code for this should be super simple!\n",
        "Try to get the best final reward possible. Warning: this could take upwards of 30 minutes to run. However, you should see it improve pretty rapidly. Unlike REINFORCE, DDPG is a much more stable algorithm :)\n",
        "\n",
        "Within 30000 timesteps and 7000 updates I was able to get to a reward of around -300\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ewNGILD088R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3cff64-b55c-4019-ab24-066a141e92ed"
      },
      "source": [
        "env = gym.make(\"Pendulum-v0\")\n",
        "alg = DDPG(env, batch_size=128, update_freq=4, lr=0.001)\n",
        "alg.learn(500000, log_freq=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "===========================\n",
            "Timesteps: 6196 Updates: 1000\n",
            "Actor Loss 35.77605438232422\n",
            "Critic Loss 0.7138609886169434\n",
            "10 Ep Avg Rew -1301.0945506449948\n",
            "===========================\n",
            "Timesteps: 10196 Updates: 2000\n",
            "Actor Loss 56.18666458129883\n",
            "Critic Loss 0.7831288576126099\n",
            "10 Ep Avg Rew -1092.9253883212305\n",
            "===========================\n",
            "Timesteps: 14196 Updates: 3000\n",
            "Actor Loss 72.72393798828125\n",
            "Critic Loss 50.39295196533203\n",
            "10 Ep Avg Rew -752.8046634416849\n",
            "===========================\n",
            "Timesteps: 18196 Updates: 4000\n",
            "Actor Loss 72.17211151123047\n",
            "Critic Loss 2.6399707794189453\n",
            "10 Ep Avg Rew -444.9932328596261\n",
            "===========================\n",
            "Timesteps: 22196 Updates: 5000\n",
            "Actor Loss 73.7801742553711\n",
            "Critic Loss 160.18409729003906\n",
            "10 Ep Avg Rew -322.5906975859953\n",
            "===========================\n",
            "Timesteps: 26196 Updates: 6000\n",
            "Actor Loss 69.15165710449219\n",
            "Critic Loss 87.8898696899414\n",
            "10 Ep Avg Rew -320.9658277644686\n",
            "===========================\n",
            "Timesteps: 30196 Updates: 7000\n",
            "Actor Loss 71.68630981445312\n",
            "Critic Loss 4.187958717346191\n",
            "10 Ep Avg Rew -230.84251398424686\n",
            "===========================\n",
            "Timesteps: 34196 Updates: 8000\n",
            "Actor Loss 72.52184295654297\n",
            "Critic Loss 86.21086120605469\n",
            "10 Ep Avg Rew -378.04171344369047\n",
            "===========================\n",
            "Timesteps: 38196 Updates: 9000\n",
            "Actor Loss 64.93859100341797\n",
            "Critic Loss 3.1133902072906494\n",
            "10 Ep Avg Rew -247.45922011838357\n",
            "===========================\n",
            "Timesteps: 42196 Updates: 10000\n",
            "Actor Loss 54.969200134277344\n",
            "Critic Loss 3.866621494293213\n",
            "10 Ep Avg Rew -216.16201672076363\n",
            "===========================\n",
            "Timesteps: 46196 Updates: 11000\n",
            "Actor Loss 58.82830047607422\n",
            "Critic Loss 431.186279296875\n",
            "10 Ep Avg Rew -220.99488378086\n",
            "===========================\n",
            "Timesteps: 50196 Updates: 12000\n",
            "Actor Loss 56.90835189819336\n",
            "Critic Loss 4.214817523956299\n",
            "10 Ep Avg Rew -315.0561419337507\n",
            "===========================\n",
            "Timesteps: 54196 Updates: 13000\n",
            "Actor Loss 58.23524475097656\n",
            "Critic Loss 4.884203910827637\n",
            "10 Ep Avg Rew -298.31162451029934\n",
            "===========================\n",
            "Timesteps: 58196 Updates: 14000\n",
            "Actor Loss 39.48033905029297\n",
            "Critic Loss 5.665915012359619\n",
            "10 Ep Avg Rew -302.0353732273654\n",
            "===========================\n",
            "Timesteps: 62196 Updates: 15000\n",
            "Actor Loss 33.42127990722656\n",
            "Critic Loss 3.5264618396759033\n",
            "10 Ep Avg Rew -248.0001269562684\n",
            "===========================\n",
            "Timesteps: 66196 Updates: 16000\n",
            "Actor Loss 51.27260971069336\n",
            "Critic Loss 2.4445083141326904\n",
            "10 Ep Avg Rew -287.99434121977276\n",
            "===========================\n",
            "Timesteps: 70196 Updates: 17000\n",
            "Actor Loss 46.8893928527832\n",
            "Critic Loss 4.847001552581787\n",
            "10 Ep Avg Rew -360.0974891862853\n",
            "===========================\n",
            "Timesteps: 74196 Updates: 18000\n",
            "Actor Loss 36.61098861694336\n",
            "Critic Loss 5.002192497253418\n",
            "10 Ep Avg Rew -229.83369738361094\n",
            "===========================\n",
            "Timesteps: 78196 Updates: 19000\n",
            "Actor Loss 43.036659240722656\n",
            "Critic Loss 2.8681118488311768\n",
            "10 Ep Avg Rew -170.41819674001863\n",
            "===========================\n",
            "Timesteps: 82196 Updates: 20000\n",
            "Actor Loss 23.78321075439453\n",
            "Critic Loss 1.8729236125946045\n",
            "10 Ep Avg Rew -195.29492487525349\n",
            "===========================\n",
            "Timesteps: 86196 Updates: 21000\n",
            "Actor Loss 33.546573638916016\n",
            "Critic Loss 2.433835744857788\n",
            "10 Ep Avg Rew -255.27610762295058\n",
            "===========================\n",
            "Timesteps: 90196 Updates: 22000\n",
            "Actor Loss 22.15202522277832\n",
            "Critic Loss 2.7959184646606445\n",
            "10 Ep Avg Rew -252.3317610341424\n",
            "===========================\n",
            "Timesteps: 94196 Updates: 23000\n",
            "Actor Loss 31.23223114013672\n",
            "Critic Loss 4.9292988777160645\n",
            "10 Ep Avg Rew -321.45634205774115\n",
            "===========================\n",
            "Timesteps: 98196 Updates: 24000\n",
            "Actor Loss 33.32954406738281\n",
            "Critic Loss 3.991772174835205\n",
            "10 Ep Avg Rew -218.7233359827463\n",
            "===========================\n",
            "Timesteps: 102196 Updates: 25000\n",
            "Actor Loss 29.923784255981445\n",
            "Critic Loss 2.5445680618286133\n",
            "10 Ep Avg Rew -347.3598684786821\n",
            "===========================\n",
            "Timesteps: 106196 Updates: 26000\n",
            "Actor Loss 17.289567947387695\n",
            "Critic Loss 1.8244895935058594\n",
            "10 Ep Avg Rew -217.28788715766805\n",
            "===========================\n",
            "Timesteps: 110196 Updates: 27000\n",
            "Actor Loss 23.789993286132812\n",
            "Critic Loss 0.9340071082115173\n",
            "10 Ep Avg Rew -222.35296101009672\n",
            "===========================\n",
            "Timesteps: 114196 Updates: 28000\n",
            "Actor Loss 38.517662048339844\n",
            "Critic Loss 1.8364641666412354\n",
            "10 Ep Avg Rew -282.34086130254553\n",
            "===========================\n",
            "Timesteps: 118196 Updates: 29000\n",
            "Actor Loss 34.373390197753906\n",
            "Critic Loss 1.3481606245040894\n",
            "10 Ep Avg Rew -227.3764838126492\n",
            "===========================\n",
            "Timesteps: 122196 Updates: 30000\n",
            "Actor Loss 30.256370544433594\n",
            "Critic Loss 1.7527353763580322\n",
            "10 Ep Avg Rew -210.6079440984809\n",
            "===========================\n",
            "Timesteps: 126196 Updates: 31000\n",
            "Actor Loss 28.04062271118164\n",
            "Critic Loss 2.116283416748047\n",
            "10 Ep Avg Rew -192.6761391557531\n",
            "===========================\n",
            "Timesteps: 130196 Updates: 32000\n",
            "Actor Loss 39.35103988647461\n",
            "Critic Loss 2.9827632904052734\n",
            "10 Ep Avg Rew -326.51444790578677\n",
            "===========================\n",
            "Timesteps: 134196 Updates: 33000\n",
            "Actor Loss 28.975440979003906\n",
            "Critic Loss 0.7176675200462341\n",
            "10 Ep Avg Rew -330.4157316727782\n",
            "===========================\n",
            "Timesteps: 138196 Updates: 34000\n",
            "Actor Loss 36.5237922668457\n",
            "Critic Loss 1.2619746923446655\n",
            "10 Ep Avg Rew -308.0566608808761\n",
            "===========================\n",
            "Timesteps: 142196 Updates: 35000\n",
            "Actor Loss 25.500289916992188\n",
            "Critic Loss 1.6545971632003784\n",
            "10 Ep Avg Rew -261.20162408850604\n",
            "===========================\n",
            "Timesteps: 146196 Updates: 36000\n",
            "Actor Loss 33.639530181884766\n",
            "Critic Loss 1.443190336227417\n",
            "10 Ep Avg Rew -267.8976727586286\n",
            "===========================\n",
            "Timesteps: 150196 Updates: 37000\n",
            "Actor Loss 32.480838775634766\n",
            "Critic Loss 1.9032135009765625\n",
            "10 Ep Avg Rew -238.03486863140634\n",
            "===========================\n",
            "Timesteps: 154196 Updates: 38000\n",
            "Actor Loss 35.67985534667969\n",
            "Critic Loss 1.0540364980697632\n",
            "10 Ep Avg Rew -344.28945741334815\n",
            "===========================\n",
            "Timesteps: 158196 Updates: 39000\n",
            "Actor Loss 24.639732360839844\n",
            "Critic Loss 0.9600198864936829\n",
            "10 Ep Avg Rew -147.50962180074845\n",
            "===========================\n",
            "Timesteps: 162196 Updates: 40000\n",
            "Actor Loss 28.267803192138672\n",
            "Critic Loss 1.7511109113693237\n",
            "10 Ep Avg Rew -233.044977567258\n",
            "===========================\n",
            "Timesteps: 166196 Updates: 41000\n",
            "Actor Loss 16.58226203918457\n",
            "Critic Loss 2.330991506576538\n",
            "10 Ep Avg Rew -253.67776324762485\n",
            "===========================\n",
            "Timesteps: 170196 Updates: 42000\n",
            "Actor Loss 26.784576416015625\n",
            "Critic Loss 0.7833117842674255\n",
            "10 Ep Avg Rew -294.4531863812371\n",
            "===========================\n",
            "Timesteps: 174196 Updates: 43000\n",
            "Actor Loss 30.15421485900879\n",
            "Critic Loss 266.9591064453125\n",
            "10 Ep Avg Rew -317.6034103663157\n",
            "===========================\n",
            "Timesteps: 178196 Updates: 44000\n",
            "Actor Loss 20.580081939697266\n",
            "Critic Loss 1.3031848669052124\n",
            "10 Ep Avg Rew -376.02536877768097\n",
            "===========================\n",
            "Timesteps: 182196 Updates: 45000\n",
            "Actor Loss 26.19027328491211\n",
            "Critic Loss 12.891156196594238\n",
            "10 Ep Avg Rew -217.26912812872965\n",
            "===========================\n",
            "Timesteps: 186196 Updates: 46000\n",
            "Actor Loss 29.300004959106445\n",
            "Critic Loss 1.5183508396148682\n",
            "10 Ep Avg Rew -323.95178214792105\n",
            "===========================\n",
            "Timesteps: 190196 Updates: 47000\n",
            "Actor Loss 19.572975158691406\n",
            "Critic Loss 1.0981084108352661\n",
            "10 Ep Avg Rew -204.57005497367118\n",
            "===========================\n",
            "Timesteps: 194196 Updates: 48000\n",
            "Actor Loss 31.092958450317383\n",
            "Critic Loss 1.3317110538482666\n",
            "10 Ep Avg Rew -306.9509400650911\n",
            "===========================\n",
            "Timesteps: 198196 Updates: 49000\n",
            "Actor Loss 32.063934326171875\n",
            "Critic Loss 1.0411148071289062\n",
            "10 Ep Avg Rew -218.1407790829923\n",
            "===========================\n",
            "Timesteps: 202196 Updates: 50000\n",
            "Actor Loss 30.9898624420166\n",
            "Critic Loss 294.4934997558594\n",
            "10 Ep Avg Rew -266.64933118915917\n",
            "===========================\n",
            "Timesteps: 206196 Updates: 51000\n",
            "Actor Loss 29.66217041015625\n",
            "Critic Loss 0.9622721076011658\n",
            "10 Ep Avg Rew -229.24676699921505\n",
            "===========================\n",
            "Timesteps: 210196 Updates: 52000\n",
            "Actor Loss 33.89345932006836\n",
            "Critic Loss 0.9250441789627075\n",
            "10 Ep Avg Rew -183.55319593351552\n",
            "===========================\n",
            "Timesteps: 214196 Updates: 53000\n",
            "Actor Loss 34.45948028564453\n",
            "Critic Loss 4.536004543304443\n",
            "10 Ep Avg Rew -285.7069209096439\n",
            "===========================\n",
            "Timesteps: 218196 Updates: 54000\n",
            "Actor Loss 26.48815155029297\n",
            "Critic Loss 0.838546097278595\n",
            "10 Ep Avg Rew -373.8925083329464\n",
            "===========================\n",
            "Timesteps: 222196 Updates: 55000\n",
            "Actor Loss 29.1320743560791\n",
            "Critic Loss 0.46468138694763184\n",
            "10 Ep Avg Rew -220.40943037588258\n",
            "===========================\n",
            "Timesteps: 226196 Updates: 56000\n",
            "Actor Loss 22.968555450439453\n",
            "Critic Loss 1.0709104537963867\n",
            "10 Ep Avg Rew -241.32923435464895\n",
            "===========================\n",
            "Timesteps: 230196 Updates: 57000\n",
            "Actor Loss 26.15880012512207\n",
            "Critic Loss 1.784907579421997\n",
            "10 Ep Avg Rew -308.05244724774366\n",
            "===========================\n",
            "Timesteps: 234196 Updates: 58000\n",
            "Actor Loss 26.533931732177734\n",
            "Critic Loss 0.7865945100784302\n",
            "10 Ep Avg Rew -273.44783980491536\n",
            "===========================\n",
            "Timesteps: 238196 Updates: 59000\n",
            "Actor Loss 26.678550720214844\n",
            "Critic Loss 0.6945086121559143\n",
            "10 Ep Avg Rew -367.3953454972705\n",
            "===========================\n",
            "Timesteps: 242196 Updates: 60000\n",
            "Actor Loss 30.456607818603516\n",
            "Critic Loss 1.5696219205856323\n",
            "10 Ep Avg Rew -313.81635141334243\n",
            "===========================\n",
            "Timesteps: 246196 Updates: 61000\n",
            "Actor Loss 32.75284957885742\n",
            "Critic Loss 0.8217064142227173\n",
            "10 Ep Avg Rew -304.6380840440964\n",
            "===========================\n",
            "Timesteps: 250196 Updates: 62000\n",
            "Actor Loss 34.26921081542969\n",
            "Critic Loss 1.4111707210540771\n",
            "10 Ep Avg Rew -309.29663505965965\n",
            "===========================\n",
            "Timesteps: 254196 Updates: 63000\n",
            "Actor Loss 29.879905700683594\n",
            "Critic Loss 1.2580493688583374\n",
            "10 Ep Avg Rew -230.43091683016138\n",
            "===========================\n",
            "Timesteps: 258196 Updates: 64000\n",
            "Actor Loss 22.986326217651367\n",
            "Critic Loss 1.150870680809021\n",
            "10 Ep Avg Rew -266.64604845364846\n",
            "===========================\n",
            "Timesteps: 262196 Updates: 65000\n",
            "Actor Loss 18.43915367126465\n",
            "Critic Loss 0.8618019819259644\n",
            "10 Ep Avg Rew -341.0929407399914\n",
            "===========================\n",
            "Timesteps: 266196 Updates: 66000\n",
            "Actor Loss 31.058408737182617\n",
            "Critic Loss 0.6086162328720093\n",
            "10 Ep Avg Rew -346.2408412338426\n",
            "===========================\n",
            "Timesteps: 270196 Updates: 67000\n",
            "Actor Loss 25.48271369934082\n",
            "Critic Loss 2.25669264793396\n",
            "10 Ep Avg Rew -258.38363292764814\n",
            "===========================\n",
            "Timesteps: 274196 Updates: 68000\n",
            "Actor Loss 27.28973388671875\n",
            "Critic Loss 3.2047364711761475\n",
            "10 Ep Avg Rew -210.4267873160677\n",
            "===========================\n",
            "Timesteps: 278196 Updates: 69000\n",
            "Actor Loss 27.06647491455078\n",
            "Critic Loss 0.6159898638725281\n",
            "10 Ep Avg Rew -244.2547820945139\n",
            "===========================\n",
            "Timesteps: 282196 Updates: 70000\n",
            "Actor Loss 16.53996467590332\n",
            "Critic Loss 1.2490583658218384\n",
            "10 Ep Avg Rew -350.6247768807379\n",
            "===========================\n",
            "Timesteps: 286196 Updates: 71000\n",
            "Actor Loss 20.13814926147461\n",
            "Critic Loss 0.6007567644119263\n",
            "10 Ep Avg Rew -300.2973290666381\n",
            "===========================\n",
            "Timesteps: 290196 Updates: 72000\n",
            "Actor Loss 28.942548751831055\n",
            "Critic Loss 0.6145316362380981\n",
            "10 Ep Avg Rew -323.8611643216403\n",
            "===========================\n",
            "Timesteps: 294196 Updates: 73000\n",
            "Actor Loss 28.537626266479492\n",
            "Critic Loss 0.6621875166893005\n",
            "10 Ep Avg Rew -185.30649647295687\n",
            "===========================\n",
            "Timesteps: 298196 Updates: 74000\n",
            "Actor Loss 42.08990478515625\n",
            "Critic Loss 0.8392516374588013\n",
            "10 Ep Avg Rew -285.1621907157702\n",
            "===========================\n",
            "Timesteps: 302196 Updates: 75000\n",
            "Actor Loss 40.37874984741211\n",
            "Critic Loss 1.4772735834121704\n",
            "10 Ep Avg Rew -230.74220259104885\n",
            "===========================\n",
            "Timesteps: 306196 Updates: 76000\n",
            "Actor Loss 39.010990142822266\n",
            "Critic Loss 23.463790893554688\n",
            "10 Ep Avg Rew -205.86590465074408\n",
            "===========================\n",
            "Timesteps: 310196 Updates: 77000\n",
            "Actor Loss 26.93294906616211\n",
            "Critic Loss 1.1346607208251953\n",
            "10 Ep Avg Rew -325.00957046829006\n",
            "===========================\n",
            "Timesteps: 314196 Updates: 78000\n",
            "Actor Loss 35.35987854003906\n",
            "Critic Loss 1.4673938751220703\n",
            "10 Ep Avg Rew -242.2748342153311\n",
            "===========================\n",
            "Timesteps: 318196 Updates: 79000\n",
            "Actor Loss 26.73843765258789\n",
            "Critic Loss 1.207227349281311\n",
            "10 Ep Avg Rew -183.58797744344062\n",
            "===========================\n",
            "Timesteps: 322196 Updates: 80000\n",
            "Actor Loss 36.386375427246094\n",
            "Critic Loss 0.5318669080734253\n",
            "10 Ep Avg Rew -247.1328655502463\n",
            "===========================\n",
            "Timesteps: 326196 Updates: 81000\n",
            "Actor Loss 19.523460388183594\n",
            "Critic Loss 0.6048036217689514\n",
            "10 Ep Avg Rew -245.79551944012823\n",
            "===========================\n",
            "Timesteps: 330196 Updates: 82000\n",
            "Actor Loss 22.44121551513672\n",
            "Critic Loss 0.2576373219490051\n",
            "10 Ep Avg Rew -313.777878476827\n",
            "===========================\n",
            "Timesteps: 334196 Updates: 83000\n",
            "Actor Loss 27.518213272094727\n",
            "Critic Loss 0.8119058012962341\n",
            "10 Ep Avg Rew -298.9201320848768\n",
            "===========================\n",
            "Timesteps: 338196 Updates: 84000\n",
            "Actor Loss 20.99061393737793\n",
            "Critic Loss 0.5871357917785645\n",
            "10 Ep Avg Rew -196.0836991591115\n",
            "===========================\n",
            "Timesteps: 342196 Updates: 85000\n",
            "Actor Loss 31.33391571044922\n",
            "Critic Loss 0.8172129392623901\n",
            "10 Ep Avg Rew -266.86727293820684\n",
            "===========================\n",
            "Timesteps: 346196 Updates: 86000\n",
            "Actor Loss 25.7036075592041\n",
            "Critic Loss 0.800652027130127\n",
            "10 Ep Avg Rew -208.75252956443472\n",
            "===========================\n",
            "Timesteps: 350196 Updates: 87000\n",
            "Actor Loss 21.3679256439209\n",
            "Critic Loss 0.3697832524776459\n",
            "10 Ep Avg Rew -226.25336357637647\n",
            "===========================\n",
            "Timesteps: 354196 Updates: 88000\n",
            "Actor Loss 21.905567169189453\n",
            "Critic Loss 2.3420798778533936\n",
            "10 Ep Avg Rew -211.46757097966255\n",
            "===========================\n",
            "Timesteps: 358196 Updates: 89000\n",
            "Actor Loss 29.771421432495117\n",
            "Critic Loss 0.6467939019203186\n",
            "10 Ep Avg Rew -195.47689814969868\n",
            "===========================\n",
            "Timesteps: 362196 Updates: 90000\n",
            "Actor Loss 16.64405059814453\n",
            "Critic Loss 1.0744695663452148\n",
            "10 Ep Avg Rew -302.24601724527736\n",
            "===========================\n",
            "Timesteps: 366196 Updates: 91000\n",
            "Actor Loss 30.451641082763672\n",
            "Critic Loss 1.0015827417373657\n",
            "10 Ep Avg Rew -284.1122598777682\n",
            "===========================\n",
            "Timesteps: 370196 Updates: 92000\n",
            "Actor Loss 16.60384750366211\n",
            "Critic Loss 0.6089309453964233\n",
            "10 Ep Avg Rew -257.3010696284623\n",
            "===========================\n",
            "Timesteps: 374196 Updates: 93000\n",
            "Actor Loss 19.808561325073242\n",
            "Critic Loss 365.0007019042969\n",
            "10 Ep Avg Rew -304.86917404628286\n",
            "===========================\n",
            "Timesteps: 378196 Updates: 94000\n",
            "Actor Loss 37.88876724243164\n",
            "Critic Loss 1.0512688159942627\n",
            "10 Ep Avg Rew -263.0886060462319\n",
            "===========================\n",
            "Timesteps: 382196 Updates: 95000\n",
            "Actor Loss 23.131778717041016\n",
            "Critic Loss 0.5371527671813965\n",
            "10 Ep Avg Rew -264.49755362569823\n",
            "===========================\n",
            "Timesteps: 386196 Updates: 96000\n",
            "Actor Loss 16.061588287353516\n",
            "Critic Loss 0.7557623386383057\n",
            "10 Ep Avg Rew -223.17268637630963\n",
            "===========================\n",
            "Timesteps: 390196 Updates: 97000\n",
            "Actor Loss 21.206342697143555\n",
            "Critic Loss 0.36472809314727783\n",
            "10 Ep Avg Rew -269.2969414324587\n",
            "===========================\n",
            "Timesteps: 394196 Updates: 98000\n",
            "Actor Loss 18.269651412963867\n",
            "Critic Loss 0.6068520545959473\n",
            "10 Ep Avg Rew -287.94107124174053\n",
            "===========================\n",
            "Timesteps: 398196 Updates: 99000\n",
            "Actor Loss 28.141626358032227\n",
            "Critic Loss 0.8391974568367004\n",
            "10 Ep Avg Rew -182.09019027965297\n",
            "===========================\n",
            "Timesteps: 402196 Updates: 100000\n",
            "Actor Loss 24.442365646362305\n",
            "Critic Loss 0.5756036043167114\n",
            "10 Ep Avg Rew -199.6532644664501\n",
            "===========================\n",
            "Timesteps: 406196 Updates: 101000\n",
            "Actor Loss 33.44795227050781\n",
            "Critic Loss 0.820212721824646\n",
            "10 Ep Avg Rew -266.0431216684165\n",
            "===========================\n",
            "Timesteps: 410196 Updates: 102000\n",
            "Actor Loss 40.523738861083984\n",
            "Critic Loss 3.5485024452209473\n",
            "10 Ep Avg Rew -272.03696206487683\n",
            "===========================\n",
            "Timesteps: 414196 Updates: 103000\n",
            "Actor Loss 22.575153350830078\n",
            "Critic Loss 0.7015209794044495\n",
            "10 Ep Avg Rew -376.12457244075915\n",
            "===========================\n",
            "Timesteps: 418196 Updates: 104000\n",
            "Actor Loss 22.807798385620117\n",
            "Critic Loss 0.5671179294586182\n",
            "10 Ep Avg Rew -209.81994999495737\n",
            "===========================\n",
            "Timesteps: 422196 Updates: 105000\n",
            "Actor Loss 21.71172332763672\n",
            "Critic Loss 1.9739172458648682\n",
            "10 Ep Avg Rew -315.5222152457144\n",
            "===========================\n",
            "Timesteps: 426196 Updates: 106000\n",
            "Actor Loss 15.943403244018555\n",
            "Critic Loss 0.9511906504631042\n",
            "10 Ep Avg Rew -276.82819592771\n",
            "===========================\n",
            "Timesteps: 430196 Updates: 107000\n",
            "Actor Loss 21.09507942199707\n",
            "Critic Loss 0.9296716451644897\n",
            "10 Ep Avg Rew -270.42910744179744\n",
            "===========================\n",
            "Timesteps: 434196 Updates: 108000\n",
            "Actor Loss 21.907875061035156\n",
            "Critic Loss 0.6418289542198181\n",
            "10 Ep Avg Rew -229.31837723471062\n",
            "===========================\n",
            "Timesteps: 438196 Updates: 109000\n",
            "Actor Loss 27.0719051361084\n",
            "Critic Loss 0.36016884446144104\n",
            "10 Ep Avg Rew -317.03771449045723\n",
            "===========================\n",
            "Timesteps: 442196 Updates: 110000\n",
            "Actor Loss 19.88306427001953\n",
            "Critic Loss 119.49156188964844\n",
            "10 Ep Avg Rew -282.82408142548246\n",
            "===========================\n",
            "Timesteps: 446196 Updates: 111000\n",
            "Actor Loss 22.59267234802246\n",
            "Critic Loss 0.7977789044380188\n",
            "10 Ep Avg Rew -283.15321636335204\n",
            "===========================\n",
            "Timesteps: 450196 Updates: 112000\n",
            "Actor Loss 30.79662322998047\n",
            "Critic Loss 0.9494668841362\n",
            "10 Ep Avg Rew -253.57764386149515\n",
            "===========================\n",
            "Timesteps: 454196 Updates: 113000\n",
            "Actor Loss 34.431026458740234\n",
            "Critic Loss 1.285150170326233\n",
            "10 Ep Avg Rew -208.21233693486556\n",
            "===========================\n",
            "Timesteps: 458196 Updates: 114000\n",
            "Actor Loss 26.194856643676758\n",
            "Critic Loss 0.3245818018913269\n",
            "10 Ep Avg Rew -218.0046972517579\n",
            "===========================\n",
            "Timesteps: 462196 Updates: 115000\n",
            "Actor Loss 21.478073120117188\n",
            "Critic Loss 4.022942066192627\n",
            "10 Ep Avg Rew -278.7647538094239\n",
            "===========================\n",
            "Timesteps: 466196 Updates: 116000\n",
            "Actor Loss 20.043970108032227\n",
            "Critic Loss 0.32700321078300476\n",
            "10 Ep Avg Rew -220.582472768888\n",
            "===========================\n",
            "Timesteps: 470196 Updates: 117000\n",
            "Actor Loss 27.123472213745117\n",
            "Critic Loss 0.33100828528404236\n",
            "10 Ep Avg Rew -297.72845136026655\n",
            "===========================\n",
            "Timesteps: 474196 Updates: 118000\n",
            "Actor Loss 26.741670608520508\n",
            "Critic Loss 0.9544640779495239\n",
            "10 Ep Avg Rew -322.23885939013314\n",
            "===========================\n",
            "Timesteps: 478196 Updates: 119000\n",
            "Actor Loss 31.182392120361328\n",
            "Critic Loss 0.3840946853160858\n",
            "10 Ep Avg Rew -254.21034297660444\n",
            "===========================\n",
            "Timesteps: 482196 Updates: 120000\n",
            "Actor Loss 27.425518035888672\n",
            "Critic Loss 0.9073522686958313\n",
            "10 Ep Avg Rew -312.284565608549\n",
            "===========================\n",
            "Timesteps: 486196 Updates: 121000\n",
            "Actor Loss 20.791934967041016\n",
            "Critic Loss 0.5028786659240723\n",
            "10 Ep Avg Rew -331.3205867209704\n",
            "===========================\n",
            "Timesteps: 490196 Updates: 122000\n",
            "Actor Loss 26.32358741760254\n",
            "Critic Loss 0.31029391288757324\n",
            "10 Ep Avg Rew -301.43266539294814\n",
            "===========================\n",
            "Timesteps: 494196 Updates: 123000\n",
            "Actor Loss 29.067014694213867\n",
            "Critic Loss 0.5763173699378967\n",
            "10 Ep Avg Rew -257.5469613871525\n",
            "===========================\n",
            "Timesteps: 498196 Updates: 124000\n",
            "Actor Loss 24.812944412231445\n",
            "Critic Loss 0.1392459273338318\n",
            "10 Ep Avg Rew -208.2933996951631\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqGin7Id1DNS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}