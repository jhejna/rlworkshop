{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MB_RL_Workshop",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhejna/rlworkshop/blob/main/MB_RL_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ez4n-EqKJI"
      },
      "source": [
        "# 0. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MWYQII3qN2c"
      },
      "source": [
        "%%bash\n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils\n",
        "# install required python dependencies (might need to install additional gym extras depending)\n",
        "pip install gym pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAhTEBJpqOe_"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(640, 480))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP9F1IS9qRPz"
      },
      "source": [
        "!echo $DISPLAY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL48oF2GqUws"
      },
      "source": [
        "# Test rendering a gym environment\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "num_ep = 0\n",
        "obs = env.reset()\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "for _ in range(100):\n",
        "    ac = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(ac)\n",
        "    img.set_data(env.render(mode='rgb_array')) \n",
        "    plt.axis('off')\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "        num_ep += 1\n",
        "env.close()\n",
        "del env\n",
        "del img\n",
        "print(\"Episodes:\", num_ep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpG3Oh4lqVZi"
      },
      "source": [
        "# 1. PETS\n",
        "For this workshop we will implement the Probablistic Ensembles with Trajectory Sampling (PETS) algorithm from \"Deep Reinforcement Learning in a Handful of Trials Using Probabilistic Models\"\n",
        "\n",
        "Here is a link to the paper: https://arxiv.org/abs/1805.12114\n",
        "\n",
        "The basic idea is this: learned dynamics models often accumulate a large amount of error that leads to bad plans. By planning through an ensemble, or collection of models, we can mitigate this error.\n",
        "\n",
        "Similar to the last workshop, we will work with the simple discrete cartpole environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fJCGDv0q7d1"
      },
      "source": [
        "# Import the required modules\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce68ETolrC6C"
      },
      "source": [
        "## a) Dynamics Models\n",
        "PETS uses an ensemble of probablistic models. What does this mean?\n",
        "1. Rather than just predicting the next state, we predict a distribution over the possible next states. In the authors implementation, this takes the form of a diagonal multivariate gaussian distribution.\n",
        "2. Rather than having a single dynamics model and sampling from it during planning, we use an ensemble to reduce error.\n",
        "\n",
        "In this section, you will implement both a guassian dynamics model in the form of an MLP and an ensemble wrapper.\n",
        "\n",
        "Here are a few things to remember when making the dynamics model:\n",
        "1. The dynamics model takes in the state and action and predicts the next state. \n",
        "2. For the guassian model, we need to predict a mean and variance for each component of the state.\n",
        "3. Dynamics models generally perform better when they predict the delta to the next state. For example `next_state = prediction + state` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8hagUNXre9w"
      },
      "source": [
        "class GaussMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, hidden_dims=[256, 256]):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        #YOUR CODE HERE\n",
        "        #HINT: out_dim is the number of dimensions of the state space\n",
        "        \n",
        "        #This defines bounds for the log of the variance\n",
        "        self.max_logvar = nn.Parameter(torch.ones(1, out_dim, dtype=torch.float32) / 2.0)\n",
        "        self.min_logvar = nn.Parameter(- torch.ones(1, out_dim, dtype=torch.float32) * 10.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #YOUR CODE HERE\n",
        "\n",
        "        # Note, authors use softplus here and bound the variance. We have provided this for you.\n",
        "        logvar = self.max_logvar - F.softplus(self.max_logvar - logvar)\n",
        "        logvar = self.min_logvar + F.softplus(logvar - self.min_logvar)\n",
        "        return mu, logvar\n",
        "    \n",
        "class GaussEnsemble(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, out_dim, ensemble_size, hidden_dims=[256, 256]):\n",
        "        #YOUR CODE HERE\n",
        "        #HINT: the ensemble is just a collection of Gaussian MLPs\n",
        "\n",
        "    def forward(self, x):\n",
        "        means, logvars = [], []\n",
        "        \n",
        "        #YOUR CODE HERE\n",
        "        #HINT: return a list of the mu's and logvar's for each model in the ensemble\n",
        "\n",
        "        return means, logvars\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eiwP9t0rjw0"
      },
      "source": [
        "## b) Environment Setup\n",
        "In model based reinforcement learning, we plan using the reward function. For example, in random shooting, trajectory optimization we will later implement, you rank different action sequences by the sum of their rewards. This means that we will need access to the reward function. By default, this isnt provided in gym, so I have provided it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Q3-cEFrhit"
      },
      "source": [
        "def reward_function(state):\n",
        "    x = state[:, 0]\n",
        "    theta = state[:, 2]\n",
        "    theta_thres = 12 * 2 * math.pi / 360\n",
        "\n",
        "    x_done = np.logical_or(x < -2.4, x > 2.4)\n",
        "    theta_done = np.logical_or(theta < -theta_thres, theta > theta_thres)\n",
        "\n",
        "    done = np.logical_or(x_done, theta_done)\n",
        "    return 1 - done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZJOcc5esTEA"
      },
      "source": [
        "Next, we need to collect data from the environment. In model based RL it's nice to package this in a single function instead of having it in your training loop. We haven't talked about how you actually plan in the environment yet, so we'll assume that you have some `act_fn` that when provided an observation will predict the next action you take. For example `action = act_fn(observation)`\n",
        "\n",
        "The `collect_data` function will take in an environment, an `act_fn`, and a number of `timesteps`, and use the `act_fn` to collect data for the specified number of timesteps. \n",
        "\n",
        "You should be able to figure out how to do this from the model free RL workshop training loop or testing functions. It's nice to log values here so you can see how your policy does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3kv0lHysRG_"
      },
      "source": [
        "def collect_data(env, act_fn, timesteps):\n",
        "    inputs = np.zeros((timesteps, 6))\n",
        "    targets = np.zeros((timesteps, 4))\n",
        "\n",
        "    obs = env.reset()\n",
        "    ep_rewards = [] #list of episode reward per episode\n",
        "    ep_reward = 0 \n",
        "    \n",
        "    #YOUR CODE HERE\n",
        "    #HINT: the \"done\" value returned by env.step() tells you how to split episodes\n",
        "    #What data do we need to train? What does the model take as input, and what does it try to predict? (ignore the variances for now)\n",
        "\n",
        "    if len(ep_rewards) == 0:\n",
        "        print(\"Total Ep Reward\", ep_reward)\n",
        "    else:\n",
        "        print(\"Last Ep Reward\", ep_rewards[-1])\n",
        "    return inputs, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3tTf9bVtVwc"
      },
      "source": [
        "## c) MPC Planning with Random Shooting\n",
        "MPC, or model predictive control, is a way of using dynamics models and a reward function to predict the action that should be taken in an environment. We will implement random shooting in particular.\n",
        "\n",
        "In random shooting, a number of random trajectories are sampled. Each trajectory is then assigned a total reward using the reward function. Finally, the tracjectory with the highest reward is selected and the first action is used.\n",
        "\n",
        "Alone, random shooting, well sucks. PETS utilizes the ensemble of probablistic models to make it not so bad. In the paper, they use a more complicated algorithm called the cross entropy method that we will not get into here :).\n",
        "\n",
        "In this section, implement random shooting for PETS. This will require the following modifications in addition to regular random shooting:\n",
        "1. Randomly sampling next states using the probablity distribution output by a given model.\n",
        "2. Compute rewards for each trajectory under all of the different models. Average the final reward across all dynamics models in the ensemble to get a better estimate.\n",
        "\n",
        "Keep in mind that the slowest part of training our model is now planning. It's crucial that your implementation of random shooting predicts next states for all samples in parallel (ie one large batch). Otherwise, you will be waiting years for everything to finish.\n",
        "\n",
        "Warning: this is by far the hardest part of the PETS algorithm :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BlrjlEVt7wV"
      },
      "source": [
        "def act(obs, models, reward_fn, horizon):\n",
        "    # Generate plans of length \"Horizon\"\n",
        "    # Each model predicts a cost over acting for horizon timesteps.\n",
        "    \n",
        "    #YOUR CODE HERE\n",
        "    #Sample a bunch of random trajectories and return the first action of the one that has the highest predicted reward\n",
        "    #HINT: The action that is input into the model is a 2-dimensional one-hot vector (i.e. [1,0] or [0,1])\n",
        "    \n",
        "    #Return the ordinal encoding of the action (the index of the one in the one-hot vector, \n",
        "    #e.g. if the selected action is [1,0] then return 0, if it's [0,1] return 1)\n",
        "    return best_ac"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNFyWk9w9mp"
      },
      "source": [
        "## d) Training Loop\n",
        "Implement the training loop for the algorithm. This should be done in two phases:\n",
        "1. Collect data\n",
        "2. Train the models\n",
        "\n",
        "Usually, models are initiallized using a few random episodes and warmed up before trying to plan.\n",
        "\n",
        "Fill in the loss. Look at the PETS paper to determine what the loss should be. Hint: It should have something for both the mean and variance. Its fine if the loss becomes negative due to the variance term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oud4FDeIxBTe"
      },
      "source": [
        "import math\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "# State Dim is 2, action is one hot of size 2\n",
        "batch_size = 64\n",
        "epochs = 2\n",
        "iterations = 15\n",
        "timesteps = 400\n",
        "horizon = 10\n",
        "\n",
        "models = [GaussMLP(4+2, 4) for i in range(5)]\n",
        "parameters = []\n",
        "for model in models:\n",
        "    parameters.extend(model.parameters())\n",
        "optim = torch.optim.Adam(parameters, lr=0.001)\n",
        "\n",
        "# Collect initial data and train.\n",
        "inputs, targets = collect_data(env, lambda x: env.action_space.sample(), timesteps)\n",
        "\n",
        "for i in range(iterations):\n",
        "    \n",
        "    for _ in range(epochs):\n",
        "        perm = np.random.permutation(len(inputs))\n",
        "        num_full_batches = len(perm) // batch_size\n",
        "        for i in range(num_full_batches + 1):\n",
        "            if i != num_full_batches:\n",
        "                inds = perm[i*batch_size:(i+1)*batch_size]\n",
        "            else:\n",
        "                inds = perm[i*batch_size:]\n",
        "            x = torch.from_numpy(inputs[inds]).float()\n",
        "            y = torch.from_numpy(targets[inds]).float()\n",
        "\n",
        "            loss = 0.0\n",
        "            for model in models:\n",
        "                mean, logvar = model(x)\n",
        "                loss += #YOUR CODE HERE\n",
        "            \n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "        print(\"Loss\", loss.item())\n",
        "    # Collect new data\n",
        "    new_inputs, new_targets = collect_data(env, lambda obs: act(obs, models, reward_function, horizon), timesteps)\n",
        "    inputs = np.concatenate((inputs, new_inputs), axis=0)\n",
        "    targets = np.concatenate((targets, new_targets), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eDppBk8xNyH"
      },
      "source": [
        "## e) Tuning\n",
        "Play with the hyper-parameters and see how high you can get the reward!\n",
        "\n",
        "Model based RL is _even_ more unstable than model free RL :). Good luck!\n",
        "\n",
        "Within a around 75 episodes I was able to get a reward of over 350, though my implementation was pretty unstable and tended to crash :P. This is the problem with using random shooting! My loss nanned a few times, this can be fixed by better controlling the variance term of the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-29lHAivxZfl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjOYfbexY3fR"
      },
      "source": [
        "# 2. VAEs and World Models\n",
        "\n",
        "In this section you'll implement a very simple VAE that could be used in some type of world model for MB RL from images. Though typically you would do this by collecting data from environments, we will do this on the MNIST dataset for simplicity.\n",
        "\n",
        "Here is the link to the original VAE Paper:\n",
        "https://arxiv.org/pdf/1312.6114.pdf\n",
        "\n",
        "And the world models paper: https://arxiv.org/abs/1803.10122\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNbxGWA7ula2"
      },
      "source": [
        "## a) Data Collection\n",
        "\n",
        "Note sometimes this tends to error because of the display environment being buggy in colab. If it crashes, go ahead and re-run the set display block at the top of the noteobok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LniJixhFZ0pn"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "data = []\n",
        "data_size = 2000\n",
        "\n",
        "obs = env.reset()\n",
        "ac = 0\n",
        "for _ in range(data_size):\n",
        "    if np.random.rand() < 0.5:\n",
        "        ac = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(ac)\n",
        "    img = Image.fromarray(env.render(mode='rgb_array'))\n",
        "    img_gray = ImageOps.grayscale(img)\n",
        "    img_resize = img_gray.resize((56,56))\n",
        "    img_crop = img_resize.crop((8,8, 48, 48))\n",
        "    data.append(np.array(img_crop))\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "env.close()\n",
        "del env\n",
        "del img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDV8RgOZb3SY"
      },
      "source": [
        "# Now Shuffle the data, and visualize some images\n",
        "data = np.array(data)\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Visualize some random images\n",
        "from matplotlib import pyplot as plt\n",
        "plt.imshow(data[0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(data[1], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(data[2], cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "212fpl7fc8_D"
      },
      "source": [
        "# Normalize data between -1 and 1\n",
        "data = data / 255.0 # Between zero and one\n",
        "data *= 2 # Between zero and two\n",
        "data -= 1 # Between -1 and 1\n",
        "\n",
        "# Expand dims of data and reorder in order to fit pytorch image conventions N x C x W x H\n",
        "data = np.expand_dims(data, 1)\n",
        "print(\"Data Shape\", data.shape)\n",
        "\n",
        "np.save('data.npy', data)\n",
        "\n",
        "# Test loading the data\n",
        "data = np.load('data.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK2dJEgHjTQL"
      },
      "source": [
        "## b) Model Architectures\n",
        "\n",
        "Next, you need to define a simple convolutional architecture for the encoder and decoder. Here are things to consider:\n",
        "\n",
        "1. Choose an appropriate size for the latent z\n",
        "2. Remember that the encoder must predict both a mean and standard deviation.\n",
        "3. The standard deviation (or variance) must be a positive quantity!\n",
        "4. The encoder and decoder architectures should closely mirror eachother.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ges7sEc2jZYs"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, latent_dim):\n",
        "        super().__init__()\n",
        "        #YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        #YOUR CODE HERE\n",
        "        return mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, output_channels, latent_dim):\n",
        "        super().__init__()\n",
        "        #YOUR CODE HERE\n",
        "        #HINT: Add a tanh at the end to bound inputs/outpus\n",
        "\n",
        "    def forward(self, x):\n",
        "        #YOUR CODE HERE\n",
        "\n",
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self, channels, latent_dim):\n",
        "        super().__init__()\n",
        "        #YOUR CODE HERE\n",
        "        #HINT: It's just an encoder + a decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        #YOUR CODE HERE\n",
        "        #Pass the input through the encoder then decoder, and return the loss\n",
        "        #HINT: the loss should have MSE (Mean Squared Error) and KLD (KL Divergence) components\n",
        "        \n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cmuv8ptq123"
      },
      "source": [
        "## c) Training Loop\n",
        "Now we code the training loop. We can do this really easily with pytorch data loaders!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B1YkshIlNB6"
      },
      "source": [
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "latent_dim = 16\n",
        "epochs = 50\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# create data loader\n",
        "data = np.load('data.npy')\n",
        "trainloader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = VAE(1, latent_dim)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in trainloader:\n",
        "        batch = batch.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Finished epoch\", epoch, \"loss\", loss.item())\n",
        "\n",
        "# Now let's generate some samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5nZ4ZMJsJyP"
      },
      "source": [
        "## d) Evaluation\n",
        "\n",
        "Now assess the quality of the learned model by sampling from the latent (sampling z, then examining x_hat)\n",
        "\n",
        "Note that an easy way of displaying the images is torchvision makegrid, it does a lot of the deprocessing work for you! \n",
        "\n",
        "\n",
        "It won't be super good, but you should be able to see the base of the cart! If you mess with the architecture I'm sure you can get it to peform better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9G-qiNTr3e-"
      },
      "source": [
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Samples!\n",
        "num_samples = 24\n",
        "samples = model.decoder(torch.randn(num_samples, latent_dim).to(device)).detach().cpu()\n",
        "sample_grid = torchvision.utils.make_grid(samples)\n",
        "sample_img = torchvision.utils.save_image(sample_grid, 'samples.png')\n",
        "plt.imshow(Image.open('samples.png'))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awxY8gtWs6Lc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}